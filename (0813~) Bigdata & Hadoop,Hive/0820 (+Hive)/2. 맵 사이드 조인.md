```
[hadoop@master ~]$ cd /usr/local/hadoop-2.7.7/sbin
[hadoop@master sbin]$ ./start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: namenode running as process 14654. Stop it first.
slave2: datanode running as process 14761. Stop it first.
slave1: ssh: connect to host slave1 port 22: No route to host
Starting secondary namenodes [secondary]
secondary: ssh: connect to host secondary port 22: No route to host
starting yarn daemons
resourcemanager running as process 15081. Stop it first.
slave1: ssh: connect to host slave1 port 22: No route to host
slave2: nodemanager running as process 15186. Stop it first.
[hadoop@master sbin]$ jps
14450 JobHistoryServer
15186 NodeManager
14761 DataNode
15081 ResourceManager
46860 Jps
14654 NameNode
[hadoop@master sbin]$ ./mr-jobhistory-daemon.sh start historyserver
historyserver running as process 14450. Stop it first.
```

```
[hadoop@master sbin]$ cd ~
[hadoop@master ~]$ ls
airline.jar  delaycount.jar  Documents  eclipse-workspace  linux_ex  Pictures  Templates  Videos
ch10         Desktop         Downloads  groupkey.jar       Music     Public    test.txt
[hadoop@master ~]$ ls ./Downloads/
2007.csv      eclipse-jee-photon-R-linux-gtk-x86_64.tar.gz  postgresql-9.2-1004.jdbc41.jar
2008.csv      hadoop-2.7.7.tar.gz
carriers.csv  jdk-8u221-linux-x64.tar.gz
[hadoop@master ~]$ hadoop fs -mkdir /data/metadata
[hadoop@master ~]$ hadoop fs -ls /data
Found 2 items
drwxr-xr-x   - hadoop supergroup          0 2019-08-18 22:41 /data/airline
drwxr-xr-x   - hadoop supergroup          0 2019-08-19 22:19 /data/metadata
[hadoop@master ~]$ hadoop fs -put ./Downloads/carriers.csv /data/metadata
[hadoop@master ~]$ hadoop fs -ls /data/metadata
Found 1 items
-rw-r--r--   1 hadoop supergroup      43758 2019-08-19 22:19 /data/metadata/carriers.csv

```

lab.hadoop.join 패키지  - MapperWithMapsideJoin 클래스

```java
package lab.hadoop.join;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.Hashtable;

import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MapperWithMapsideJoin extends
		Mapper<LongWritable, Text, Text, Text> {

	private Hashtable<String, String> joinMap 
	                     = new Hashtable<String, String>();

	// map 출력키
	private Text outputKey = new Text();

	@Override
	public void setup(Context context) throws IOException, 
	                                       InterruptedException {
		try {
			// 분산캐시 조회
			Path[] cacheFiles = DistributedCache.getLocalCacheFiles(context
					.getConfiguration());
			// 조인 데이터 생성
			if (cacheFiles != null && cacheFiles.length > 0) {
				String line;
				String[] tokens;
				BufferedReader br = new BufferedReader(new FileReader(
						cacheFiles[0].toString()));
				try {
					while ((line = br.readLine()) != null) {
						tokens = line.toString().replaceAll("\"", "")
								.split(",");
						joinMap.put(tokens[0], tokens[1]);
					}
				} finally {
					br.close();
				}
			} else {
				System.out.println("### cache files is null!");
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {

		if (key.get() > 0) {
			// 콤마 구분자 분리
			String[] colums = value.toString().split(",");
			if (colums != null && colums.length > 0) {
				try {
					outputKey.set(joinMap.get(colums[8]));
					context.write(outputKey, value);
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	}
}
```

MapsideJoin.java

```java
package lab.hadoop.join;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class MapsideJoin extends Configured implements Tool {

	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(), args)
				.getRemainingArgs();
		// 입력출 데이터 경로 확인
		if (otherArgs.length != 3) {
			System.err.println("Usage: MapsideJoin <metadata> <in> <out>");
			System.exit(2);
		}
		
		Configuration conf = new Configuration();

		// 파일 시스템 제어 객체 생성
		FileSystem hdfs = FileSystem.get(conf);
		// 경로 체크
		Path path = new Path(args[2]);
		if (hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		// Job 이름 설정
		Job job = new Job(getConf(), "MapsideJoin");

		// 분산 캐시 설정
		DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(),
				job.getConfiguration());

		// 입출력 데이터 경로 설정
		FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

		// Job 클래스 설정
		job.setJarByClass(MapsideJoin.class);
		// Mapper 설정
		job.setMapperClass(MapperWithMapsideJoin.class);
		// Reducer 설정
		job.setNumReduceTasks(0);

		// 입출력 데이터 포맷 설정
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		// 출력키 및 출력값 유형 설정
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		job.waitForCompletion(true);
		return 0;
	}

	public static void main(String[] args) throws Exception {
		// Tool 인터페이스 실행
		int res = ToolRunner.run(new Configuration(), new MapsideJoin(), args);
		System.out.println("## RESULT:" + res);
	}
}
```

---

드라이버 클래스 : mapsidjoin.jar 파일 생성

hadoop jar ./mapsidejoin.jar  /data/metadata/carriers.csv /data/airline/ /output/mapjoin/

