### 세이프 모드

- 출력 장소에 지정한 데이터셋이 이미 존재할 경우 어떻게 처리할지 결정

| 세이프 모드          | 효과                                                         |
| -------------------- | ------------------------------------------------------------ |
| SaveMode.ErrorExists | 예외를 발생시킨다(디폴트)                                    |
| SaveMode.Append      | 기존 데이터셋에 덧붙인다                                     |
| SaveMode.Overwrite   | 기존 데이터셋을 덮어쓴다.                                    |
| SaveMode.Ignore      | 기존 데이터셋을 변경하지 않는다.(=create tabe if not exists) |

- 세이프 모드 하려면 **org.apache.spark.sql.SaveMode**임포트 해야한다.

```cmd
hadoop fs -mkdir /output/dessert_json
```

```scala
dessertDF.write.save("/output/dessert_json") //예외 AnalysisException 발생 (이미 존재함)

```

```scala
import org.apache.spark.sql.SaveMode
dessertDF.write.format("json").mode(SaveMode.Overwrite).save("output/dessert_json") //예외발생 안한다!

val dessertDF2=dfReader.format("json").load("/output/dessert_json")
dessertDF2.orderBy($"kcal").show(4)
+------+---------------+-----+----+
|menuId|           name|price|kcal|
+------+---------------+-----+----+
|  D-15|  커스터드 푸딩| 2000| 120|
|   D-8|  바닐라 젤라또| 3600| 131|
|  D-17|    치즈 수플레| 2200| 160|
|  D-14|바나나 크레이프| 3300| 220|
+------+---------------+-----+----+
only showing top 4 rows

```



#### 명시적으로 스키마 정보 부여하기

- 데이터에 대한 스키마 정보를 나타내는 API, StuctType은 데이터프레임의 레코드에 대한 구조 정보를 나타내며, 내부에 여러 개의 StructField를 갖는 형태로 정의

```scala
import java.math.BigDecimal
case class DecimalTypeContainer(data: BigDecimal)
val bdContainerDF=sc.parallelize(
List(new BigDecimal("123456.6789999999999"))).map(data=>DecimalTypeContainer(data)).toDF
bdContainerDF.printSchema

bdContainerDF.show(false)//data문자열의 길이가 20을 넘을 경우에도 생략하지 않고 표시하려는 것
+--------------------+
|data                |
+--------------------+
|123456.6789999999999|
+--------------------+

bdContainerDF.write.format("orc").save("/output/bdContainerORC")
val bdContainerORCDF=spark.sqlContext.read.format("orc").load("/output/bdContainerORC")
bdContainerORCDF.printSchema
bdContainerORCDF.show(false)
+-------------------------+
|data                     |
+-------------------------+
|123456.678999999999900000|
+-------------------------+

bdContainerDF.write.format("json").save("/output/bdContainerJSON")
val bdContainerJSONDF= spark.sqlContext.read.format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema

bdContainerJSONDF.show(false)
+----------+
|data      |
+----------+
|123456.679|
+----------+

import org.apache.spark.sql.types.DataTypes._
val schema=createStructType(Array(createStructField("data",createDecimalType(38,18),true)))

val bdContainerJSONDF=spark.sqlContext.read.schema(schema).format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema

bdContainerJSONDF.show(false)
+-------------------------+
|data                     |
+-------------------------+
|123456.678999999999900000|
+-------------------------+
//이처럼 스키마를 명시적으로 설정함으로 형변환에 따른 오차 없이 파일 내용을 정확히 DataFrame으로 변환 가능
```

### 파티셔닝

- where절이나 where 메서드의 필터링 조건식에 파티션을 지정하여 해당 파티션 이외의 데이터를 읽지 않게 함

```scala
import org.apache.spark.sql.types.DataTypes._
val priceRangeDessertDF=dessertDF.select(((($"price"/1000)cast IntegerType)*1000)as "price_range",dessertDF("*"))



//priceRangeDessertDF를 파티셔닝하지 않고 출력
priceRangeDessertDF.write.format("parquet").save("/output/price_range_dessert_parquet_non_partitioned")

val nonPartitionedDessertDF=spark.sqlContext.read.format("parquet").load("/output/price_range_dessert_parquet_non_partitioned")
nonPartitionedDessertDF.where($"price_range">=5000).explain
== Physical Plan ==
*(1) Project [price_range#243, menuId#244, name#245, price#246, kcal#247]
+- *(1) Filter (isnotnull(price_range#243) && (price_range#243 >= 5000))
   +- *(1) FileScan parquet [price_range#243,menuId#244,name#245,price#246,kcal#247] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/output/price_range_dessert_parquet_non_partitioned], PartitionFilters: [], PushedFilters: [IsNotNull(price_range), GreaterThanOrEqual(price_range,5000)], ReadSchema: struct<price_range:int,menuId:string,name:string,price:int,kcal:int>

```

```scala
//priceRangeDessertDF를 파티셔닝하고 출력
priceRangeDessertDF.write.format("parquet").partitionBy("price_range").save("/outputprice_range_dessert_parquet_partitioned")
val partitionedDessertDF=spark.sqlContext.read.format("parquet").load("/outputprice_range_dessert_parquet_partitioned")
partitionedDessertDF.where($"price_range">=5000).explain
                                                                                                     == Physical Plan ==
*(1) FileScan parquet [menuId#259,name#260,price#261,kcal#262,price_range#263] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/outputprice_range_dessert_parquet_partitioned], PartitionCount: 2, PartitionFilters: [isnotnull(price_range#263), (price_range#263 >= 5000)], PushedFilters: [], ReadSchema: struct<menuId:string,name:string,price:int,kcal:int>

```

### 구조화된 데이터셋을 테이블로 다루기

`create table 테이블명 using <해당 포맷의 프로바이더명>options(옵션,옵션....)

- using절에는 다루고자 하는 포맷에 대응하는 프로바이더의 이름 지정 . 아래를 참고

| 파일 포맷 | 대응 프로바이더명             |
| --------- | ----------------------------- |
| Parquet   | org.apache.spark.sql.parquet  |
| ORC       | org.apache.spark.sql.hive.orc |
| JSON      | org.apache.spark.sql.json     |

```cmd
[hadoop@master ~]$ spark-sql
#하면 들어가진다 spark-sql
```

```scala
create table dessert_tbl_json USING org.apache.spark.sql.json OPTIONS(path 'output/dessert_json');

SELECT name, price FROM dessert_tbl_json LIMIT 3;
초콜릿 파르페   4900
푸딩 파르페     5300
딸기 파르페     5200

```

### 테이블 캐시

- DataFrame이나 테이블을 이그제큐터에 캐시 가능. 

```scala
df.cache()// DataFrame을 캐시에 저장, df는 DataFrame을 나타낸다.
spark.sqlContext.cacheTable("tbl")//테이블을 캐시에 저장, sqlContext는 SQLContext의 인스턴스를 tbl은 테이블명을 나타낸다.

df.unpersist// DataFrame 캐시에서 삭제
spark.sqlContext.uncacheTable("tbl")//테이블을 캐시에서 삭제

```

## 스파크 스트리밍

### 스트림처리

- 7장에서는 스트림처리를 구현하기 위해 스트리밍 라이브러리가 있다.

- 짧은 시간 발생한 데이터를 반복적으로 처리하는 것 

- 예를 들어 혈액냉장고의 온도 센서 확인 등

- 갑작스러운 변화의 경향 파악이 쉽다.

  

### 스트림 데이터 란?

- 스토리지에 저장된 하나의 큰 데이터가 아닌 '반영구적으로 계속 생성되는 데이터'
- 짧은 시간 간격으로 생성되는 것이 특정

### 스파크 스트리밍

- RDD변환처리로 데이터처리 구현
- 조각을 나누어 입력 데이터를 구성, 반복 처리한다는 것이 특징

### DStrema 변환

1. map
   - 원래의 DStream을 변환하여 새로운 DStream 생성하는 API다.
2. flatMap
   - DStream의 각 요소에 대해서 함수를 적용하고, 다차원 컬렉션을 한 차원 풀어 내린 요소들로 이루어진 DStream생성. 인수로는 변환처리가 정의된 함수를 건네준다.
3. filter
   - 원래의 DStream에서 조건을 만족하는 요소만을 남긴 DStream을 생성. 인수로는 조건을 확인하기 위한 함수를 건네준다.

#### 스파크 스트리밍 동작 확인

1. 

### 넷캣(Netcat)

1. TCP나 UDP프로토콜을 사용하는 네트워크 연결에서 데이터를 읽고 쓰는 간단한 유틸리티 프로그램

   nc는 network connection에 읽거나 쓴다.

   Network connection에서 raw-data read,write를 할 수 있는 유틸리티프로그램으로 원하는 포트로 원하는 데이터를 주고받을 수 있는 특징으로  해킹에도 사용되며 컴퓨터 포렌식에 있어서 라이브시스템의 데이터를 손쉽게 가져오기 위해 사용

```cmd
#마스터에서
[hadoop@master ~]$ nc -l 9999
#슬레이브에서
[hadoop@slave ~]$ nc 마스터ipaddress(000.000.000.000) 9999
#하면 둘이 연결이 된다! 아무거나 작성해 보자

#마스터에서 (파일은 만들어 놓아야 한다.)
nc -l 9999 > ./listen.txt
#슬레이브에서
nc 192.168.255.130 9999 < ./input.txt


```

- 마스터에서 ` nc -lk 9999' 상태에서(slave연결 끊고)
- /usr/local/hadoop2.7.7/ 에서 spark-shell (--master local[*])실행 후

```scala
import org.apache.spark.{SparkContext,SparkConf}
import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level,Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc=new StreamingContext(sc,Seconds(10))
val lines=ssc.socketTextStream("localhost",9999,StorageLevel.MEMORY_AND_DISK_SER)
val words=lines.flatMap(_.split(" "))
val wordCounts=words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()


```

2. 클러스터 환경에 애플리케이션 배포

```scala
//hadoop fs -mkdir /data/sample_dir
import org.apache.spark.{SparkContext,SparkConf}
import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level,Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc=new StreamingContext(sc,Seconds(10))
val lines=ssc.textFileStream("/data/sample_dir/")
val words=lines.flatMap(_.split(" "))
val wordCounts=words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()

//hadoop fs -put /usr/local/spark/README.md /data/sample_dir
//대상 파일을 읽어서 wordCount처리 하는 중! 파일이 업로드 되면 바로 읽는다.
```

#### 평균데이터 계산

- 스파크 스트리밍을 이용해 샘플 데이터로 
- [데이터](http://archive.ics.uci.edu/ml/index.php) 에서 human activity ~를 data folder로 zip으로 받자

```cmd
[hadoop@master Downloads]$ unzip UCI\ HAR\ Dataset.zip
#파일이 이름에 공백이 있으므로 unzip으로 풀어주자.
[hadoop@master Downloads]$ mv UCI\ HAR\ Dataset UCI_HAR_Dataset
#공백이 있어 불편함으로 이름을 바꿔주자 UCI_HAR_Dataset으로 이름이 바뀐것을 확인한다.



```



## 머신러닝:MLlib

- MLlib은 통계 처리나 머신러닝을 구현하기 위한 라이브러리로, 내부적으로 스파크 코어의 기본 API를 이용하므로 분산처리 기능 자연스럽게 사용 가능.
- 스파크 코어에는 스파크 SQL, MLlib, 스파크 스트리밍, 그래프 X가 있다.

#### 통계 처리, 머신러닝

- 특정 데이터로부터 수학적 기법을 이용하여 그 성질을 끄집어내는 처리
- 과거의 데이터를 이용하여 미래 데이터에 대한 예측을 하는 처리
- 대부분 행렬계산 라이브러리
- 예로는 
  1. 텍스트 정보로부터 긍정적/부정적 표현 여부 판정
  2. 인터넷 쇼핑몰 등에서 사용자의 일정한 패턴을 부석하여 상품 추천
  3. 등등
- [스파크공식사이트](https://spark.apache.org/)에서 최신 정보가 추가 되고 있다.

#### 기본 요소

1. 벡터(Vector)- 1차원 데이터 다루는 데이터 타입

   - LocalVector

     > 기본적인 벡터 타입으로 밀집 벡터와 희소 벡터 클래스를 이용

   - LabeledPoint

     > LocalVector+레이블 
     >
     > 알고리즘이용시 독립변수와 종속 변수 함께 보존시 사용

2. 행렬(Matrix)-1개 이상의 벡터로 구성되는 행렬 형식 데이터 타입

   - LocalMatrix

     > 한 대의 호스트로 행렬 형식 데이터 다루기 위한 데이터 타입

   - DistributedMatrix

     > 대규모 데이터셋을 분산처리하기 위한 행렬 형식 데이터 타입.

#### 분류와 회귀

- 미리 학습해둔 학습 데이터를 통해 분류모델 작성, 따로 처리된 데이터에 모델 을 적용할 수 있다. 

#### 협업 필터링

- 고객/ 상품에 관한 구매 이력 등의 데이터를 입력데이터로 작성하고 이를 이용하여 관련 정보 추천을 한다.

#### 클러스터링

- 데이터가 주어졌을 때 여러 개의 집합으로 나누는 것. 예를 들어 정보성 메일을 받을 때 고객의 특성을 바탕으로 분류하고 이에 적합한 메일을 보내는 것

#### 차원 축소

- 차원 축소 방법으로 SVD,PCA를 이용 할 수 있다. 이를 이용하여 어떤 정보든 형태적으로 유지하면서 데이터 차원을 축소 가능
- 이해하기 쉬운 저차원으로 변환하기 위해, 차원이 너무 높이 알고리즘 분석이 어려운 경우 사용

#### 특징 추출/변환

- 문서 특징 추출하는 방법중 하나로 벡터화 방법으로 TF-IDF, Word2Vec를 사용 가능.
- 수치 데이터 뿐만이 아닌 텏트 데이터도 입력으로 사용가능

#### 빈발 패턴 마이닝

- FP-growth, 연관성 규칙 마이닝, 순차 패턴 마이닝 등을 사용 가능. 이를 이용하여 '빵을 사는 사람은 우유를 사는 경우가 많다' 와 같은 패턴 발견 가능. 

#### PMML 익스포트 기능

#### PMML익스포트 기능

- 스파크 이외의 툴에서 이 모델을 사용할 수 있게 하는 것.

#### K-means 개요

- K-means의 샘플용 데이터는 스파크 본체에 포함

```cmd
cd usr/local/spark
cd data/mllib
cat kmeans_data.txt
```

- 로 확인 가능

```scala
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
val sparkHome=sys.env("SPARK_HOME")
val data=sc.textFile("file://"+sparkHome+"/data/mllib/kmeans_data.txt")
val parsedData=data.map{s=>
Vectors.dense(s.split(' ').map(_.toDouble))}.cache()


val numClusters=2
val numIterations=20
val clusters=KMeans.train(parsedData,numClusters,numIterations)

clusters.k

clusters.clusterCenters

val vec1=Vectors.dense(0.3,0.3,0.3)
clusters.predict(vec1)

val vec2=Vectors.dense(8.0,8.0,8.0)
clusters.predict(vec2)

parsedData.foreach(vec=>
                  println(vec+"=>"+clusters.predict(vec)))

val predictedLabels=parsedData.map(vector=> clusters.predict(vector))
predictedLabels.saveAsTextFile("output/kmeans")
//경로 지정을 안했으므로 하둡에 /user/hadoop/output/kmenas에 있다.
clusters.save(sc, "kmeans_model")


```

```cmd
[hadoop@master ~]$ hadoop fs -cat /user/hadoop/kmeans_model/metadata/part-00000 
{"class":"org.apache.spark.mllib.clustering.KMeansModel","version":"2.0","k":2,"distanceMeasure":"euclidean","trainingCost":0.11999999999994547}
#현재 사용하는 모델의 조건? 등을 알 수 있다.
```



##### 만약 클러스터의 갯수를 모른다면

```scala
val WSSSE=clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors="+WSSSE) Within Set Sum of Squared Errors=0.11999999999994547
```

#### 단어의 벡터화(한국어)

##### 문서에서 단어 추출(형태소 분석)

- 한국어는 단순한 공백으로 분할로는 충분하지 않기에 의미를 갖는 최소한의 단위인 형태소로 분할해야 한다.
- 형태소 분석은 MLlib에 포함되어있지는 않다.
- twitter-korean-text를 이용한다.

[트위터메이븐 한글 자르파일](https://mvnrepository.com/artifact/com.twitter.penguin/korean-text/4.4.4) 다운!

```cmd
$spark_HOME/jars폴더에 mv
```



```scala
libraryDependencies += "com.twitter.penguin" %% "korean_text" % "4.0"
```

```scala
spark-shell --master yarn \
--packages com.twitter.penguin:korean-text:4.4.4

spark-shell \
--master yarn \
--packages com.twitter.penguin:korean-text:4.4.4 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
// 위에 \표시는 엔터 표시임으로 \ 지우고 한줄로 작성해도 된다spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
//안되면 yarn말고 local로 하자
import java.io.StringReader
import org.apache.spark.mllib.feature.{Word2Vec,Word2VecModel}
import com.twitter.penguin.korean.TwitterKoreanProcessor
import com.twitter.penguin.korean.tokenizer.KoreanTokenizer.KoreanToken
import org.apache.spark.mllib.linalg.Vectors

val sentence="이 책은 무슨 책 입니까"
val normalized: CharSequence=TwitterKoreanProcessor.normalize(sentence)

//토크나이즈
val tokens: Seq[KoreanToken]=TwitterKoreanProcessor.tokenize(normalized)

//어근 추출
val stemmed: Seq[KoreanToken]=TwitterKoreanProcessor.stem(tokens)
```





##### 실제 파일로 찾아보자

```scala
val input=sc.textFile("")
```



spark-shell --master local --packages com.twitter.penguin:korean-text:4.4.4 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer

spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.4.4

### 참고

#### 파일 찾기

```cmd
[hadoop@master ~]$ hadoop fs -find / -name kmeans -print
```