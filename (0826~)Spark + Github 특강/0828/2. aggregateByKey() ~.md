### aggregateByKey()

### pipe()

- Pipe를 이용하면 데이터를 처리하는 과정에서 외부 프로세스를 활용할 수 있습니다.

### coalesce(), repartition()

### repartitionAndSortWithinPartitions()

- RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리하고 <u>각 파티션 단위로 정렬</u>을 수행한 뒤 이 결과로 <u>새로운 RDD를 생성</u>해 주는 메서드

- 데이터가 <u>키와 값 쌍</u>으로 구성돼 있어야 하고 메서드를 실행할 때 각 데이터가 어떤 파티션에 속할지 결정하기 위한  <u>파티셔너(Partitioner)를 설정</u>해야 합니다.

- 파티셔너는 각 데이터의 키 값을 이용해 데이터가 속할 파티션을 결정하게 되는데, 이때 키 값을 이용한 정렬도 함께 수행됩니다.

- 파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬을 각각 따로 따로 하는 것에 비해 더 높은 성능을 발휘할 수 있습니다.

- foreachPartition() 은 RDD의 파티션 단위로 특정 함수를 실행해 주는 메서드

  ```scala
  scala> val data = for (i <- 1 to 10) yield (r.nextInt(100), "-")
  data: scala.collection.immutable.IndexedSeq[(Int, String)] = Vector((90,-), (19,-), (99,-), (83,-), (55,-), (90,-), (16,-), (27,-), (69,-), (54,-))
  
  scala> val rdd1 = sc.parallelize(data)
  rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at <console>:27
  
  scala> val rdd2 = rdd1.repartitionAndSortWithinPartitions(new HashPartitioner(3))
  rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[1] at repartitionAndSortWithinPartitions at <console>:26
  
  scala> rdd2.foreachPartition(it => { println("========"); it.foreach(v=>println(v)) })
  [Stage 0:===========================================================(1 + 0) / 1]========
  (27,-)
  (54,-)
  (69,-)
  (90,-)
  (90,-)
  (99,-)
  ========
  (16,-)
  (19,-)
  (55,-)
  ========
  (83,-)
  ```

  

p112 

### RDD 요소 정렬하기

```scala
scala> val textRDD=sc.textFile("/data/spark/simple-words.txt
<console>:1: error: unclosed string literal
val textRDD=sc.textFile("/data/spark/simple-words.txt
                        ^

scala> val textRDD=sc.textFile("/data/spark/simple-words.txt")
textRDD: org.apache.spark.rdd.RDD[String] = /data/spark/simple-words.txt MapPartitionsRDD[3] at textFile at <console>:25

scala> val wordCandidateRDD=textRDD.flatMap(_.split("[ ,.]"))
wordCandidateRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26

scala> val wordRDD= wordCandidateRDD.filter(_.matches("""\p{Alnum}+"""))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:26

scala> val wordAndOnePariRDD=wordRDD.map((_,1))
wordAndOnePariRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at <console>:26

scala> val wordAndCountRDD = wordAndOnePariRDD.reduceByKey(_+_)
wordAndCountRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at reduceByKey at <console>:26

scala> val countAndWordRDD=wordAndCountRDD.map {wordAndCount =>
     | (wordAndCount._2, wordAndCount._1)
     | }
countAndWordRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[8] at map at <console>:26

scala> val sortedCWRDD =countAndWordRDD.sortByKey(false)
sortedCWRDD: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[9] at sortByKey at <console>:26

scala> val sortedWCRDD=sortedCWRDD.map {countAndWord =>
     | (countAndWord._2,countAndWord._1)
     | }
sortedWCRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:26

scala> val sortedWCArray = sortedWCRDD.collect
sortedWCArray: Array[(String, Int)] = Array((cat,4), (bear,3), (rabbit,3), (tiger,2), (dog,2), (100,1), (org,1))

scala> sortedWCArray.foreach(println)
(cat,4)
(bear,3)
(rabbit,3)
(tiger,2)
(dog,2)
(100,1)
(org,1)
```

### RDD 선두로부터 요소 꺼내기

[hadoop@master ~]$ cd spark-simple-app/
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example
[hadoop@master example]$ vi WordCountTop3.scala

```scala
package lab.spark.example

import org.apache.spark.{SparkConf,SparkContext}

object WordCountTop3 {	
	def main(args: Array[String]) {
	require(args.length >= 1,
	"드라이버 프로그램의 인수에 단어를 세고자 하는 " + "파일의 경로를 지정해 주세요.")
	val conf= new SparkConf
	val sc = new SparkContext(conf)

	try {
	//모든 단어에 대해(단어, 등장횟수) 형의 튜플을 만든다
	val filePath=args(0)
	val wordAndCountRDD=sc.textFile(filePath)
				.flatMap(_.split("[ ,.]"))
				.filter(_.matches("""\p{Alnum}+"""))
				.map((_,1))
				.reduceByKey(_+_)

	//등장횟수가 가장 많은 단어 세개를 찾느다
	val top3Words = wordAndCountRDD.map {
	case (word,count) => (count,word)
	}.sortByKey(false).map {
	case (count,word) => (word,count)
	}.take(3)

	//등장횟수가 가장 많은 단어 상위 3개를 표준 출력으로 표시
	top3Words.foreach(println)
	}finally {
	sc.stop()
}
}
}

```

[hadoop@master spark-simple-app]$ sbt assembly

[hadoop@master spark-simple-app]$ spark-submit --master local --class lab.spark.example.WordCountTop3 --name WordCountTop3 ~/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/spark/simple-words.txt

(cat,4)
(bear,3)
(rabbit,3)

### CSV 

```scala
scala> def createSalesRDD(csvFile:String)={
     |     val logRDD=sc.textFile(csvFile)
     |     logRDD.map { record=>
     |     val splitRecord= record.split(",")
     |     val productId = splitRecord(2)
     |     val numOfSold = splitRecord(3).toInt
     |     (productId,numOfSold)
     |     }
     | }
createSalesRDD: (csvFile: String)org.apache.spark.rdd.RDD[(String, Int)]

scala> val salesOctRDD = createSalesRDD("/csv/sales-october.csv")
salesOctRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[13] at map at <console>:27

scala> val salesNovRDD = createSalesRDD("/csv/sales-november.csv")
salesOctRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[16] at map at <console>:27

scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD

scala> def createOver50SoldRDD(rdd:RDD[(String,Int)]) = {
     | rdd.reduceByKey(_+_).filter(_._2 >=50)
     | }
createOver50SoldRDD: (rdd: org.apache.spark.rdd.RDD[(String, Int)])org.apache.spark.rdd.RDD[(String, Int)]

scala> val octOver50SoldRDD= createOver50SoldRDD(salesOctRDD)
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/csv/sales-november.csv

[hadoop@master ~]$ hadoop fs -mkdir csv/
[hadoop@master ~]$ hadoop fs -put csv/products.csv /csv
[hadoop@master ~]$ hadoop fs -put csv/sales-november.csv /csv
[hadoop@master ~]$ hadoop fs -put csv/sales-october.csv /csv
[hadoop@master ~]$ hadoop fs -ls /csv
Found 3 items
-rw-r--r--   1 hadoop supergroup        454 2019-08-27 22:31 /csv/products.csv
-rw-r--r--   1 hadoop supergroup        907 2019-08-27 22:31 /csv/sales-november.csv
-rw-r--r--   1 hadoop supergroup        938 2019-08-27 22:32 /csv/sales-october.csv


scala> val octOver50SoldRDD= createOver50SoldRDD(salesOctRDD)
octOver50SoldRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[18] at filter at <console>:26

scala> val salesNovRDD = createSalesRDD("/csv/sales-november.csv")
salesNovRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[21] at map at <console>:27

scala> val novOver50SoldRDD= createOver50SoldRDD(salesNovRDD)
novOver50SoldRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[23] at filter at <console>:26

scala> val bothOver50SoldRDD=octOver50SoldRDD.join(novOver50SoldRDD)
bothOver50SoldRDD: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[26] at join at <console>:29

scala> bothOver50SoldRDD.collect.foreach(println)
(8,(72,72))
(15,(51,51))

scala> val over50SoldAndAmountRDD =bothOver50SoldRDD.map {
     | case (productId,(octAmount, novAmount)) =>
     | (productId, octAmount + novAmount)
     | }
over50SoldAndAmountRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[27] at map at <console>:27

scala> over50SoldAndAmountRDD.collect.foreach(println)
(8,144)
(15,102)

```

### broadcast 변수

![](C:\Users\HK\Documents\GitHub\TIL\(0826~)Spark + Github 특강\0828\assets\broadcast.png)

```scala
scala> import scala.collection.mutable.HashMap
import scala.collection.mutable.HashMap

scala> import java.io.{BufferedReader, InputStreamReader}
import java.io.{BufferedReader, InputStreamReader}

scala> import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.conf.Configuration

scala> import org.apache.hadoop.fs.{FileSystem,Path}
import org.apache.hadoop.fs.{FileSystem, Path}

scala> val productsMap = new HashMap[String, (String,Int)]
productsMap: scala.collection.mutable.HashMap[String,(String, Int)] = Map()

scala> val hadoopConf = new Configuration
hadoopConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml

scala> val fileSystem  = FileSystem.get(hadoopConf)
fileSystem: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-589117007_1, ugi=hadoop (auth:SIMPLE)]]

scala> val inputStream = fileSystem.open(new Path("/csv/products.csv"))
inputStream: org.apache.hadoop.fs.FSDataInputStream = org.apache.hadoop.hdfs.client.HdfsDataInputStream@66ba0395

scala> val productsCSVReader = new BufferedReader(new InputStreamReader(inputStream))
productsCSVReader: java.io.BufferedReader = java.io.BufferedReader@362d45ce

scala> var line = productsCSVReader.readLine
line: String = 0,송편(6개),12000

scala> while(line!=null){
     | val splitLine = line.split(",")
     | val productId=splitLine(0)
     | val productName=splitLine(1)
     | val unitPrice = splitLine(2).toInt
     | productsMap(productId) = (productName,unitPrice)
     | line = productsCSVReader.readLine
     | }

scala> productsCSVReader.close()

scala> val broadCastedMap =sc.broadcast(productsMap)
broadCastedMap: org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[String,(String, Int)]] = Broadcast(13)

scala> val resultRDD = over50SoldAndAmountRDD.map{
     | case (productId,amount) =>
     | val productsMap = broadCastedMap.value
     | val (productName, unitPrice) =productsMap(productId)
     | (productName, amount, amount * unitPrice)
     | }
resultRDD: org.apache.spark.rdd.RDD[(String, Int, Int)] = MapPartitionsRDD[28] at map at <console>:35

scala> resultRDD.collect.foreach(println)
(강정(10개),144,2160000)
(생과자(10개),102,1734000)

```

### 클러스터 환경에서의 공유 변수

- 클러스터 환경에서 프레임워크들은 다수의 프로세스가 공유할 수 있는 읽기 자원과 쓰기 자원을 설정할 수 있도록 지원합니다.
- 하둡은 분산캐시와 카운터를, 스파크는 브로드캐스트 변수와 **어큐물레이터(Accumulators)**를 제공하고 있습니다.
- 하둡의 분산 캐시는 단순히 대용량 파일을 전체 노드에서 쉽게 접근할 수 있게 하거나 단순히 숫자(카운트)를 증가시키는 것이 목적인 데 반해 // 
  스파크의 공유 변수는 단어 그대로 “<u>읽거나 쓸 수 있는 공유 변수</u>"의 의미로서 사용 목적에 따라 좀 더 범용적인 목적으로 활용할 수 있습니다.

#### 브로드캐스트 변수

- 스파크 잡이 실행되는 동안 클러스터 내의 <u>모든 서버에서 공유 할 수 있는 읽기 전용 자원을 설정</u>할 수 있는 변수
- 공유하고자 하는 데이터를 포함한 <u>오브젝트를 생성</u>하고 ( 직렬화 가능한 객체 )
- 오브젝트를 스파크 컨텍스트의 <u>broadcast()</u> 의 인자로 지정해 해당 메서드를 실행합니다
- 생성된 브로드캐스트 변수를 <u>value()</u> 를 통해 접근할 수 있습니다

#### Accumulator

- <u>쓰기</u> 동작을 위한 것
- 클러스터 내의 모든 서버가 공유하는 쓰기 공간을 제공
- 각 서버에서 발생하는 특정 <u>이벤트의 수를 세거나 관찰하고 싶은 정보</u>를 모아 두는 등의 용도로 활용
- org.apache.spark.util.AccumulatorV2 클래스를 상속받은 클래스를 정의하고 이 클래스의 인스턴스를 생성합니다.
- accumulator 인스턴스를 스파크 컨텍스트가 제공하는 <u>register()</u>를 이용해 등록합니다
- 스파크에서는 자주 사용되는 몇 가지 데이터 타입에 대한 어큐물레이터를 미리 정의해뒀습니다.
  - LongAccumulator, DoubleAccumulator, CollectionAccumulator
- 어큐뮬레이터를 증가시키는 동작은 클러스터의 모든 데이터 처리 프로세스에서 가능하지만  데이터를 읽는 동작은 <u>드라이버 프로그램 내</u>에서만 가능하다 
- RDD의 트랜스포메이션이나 액션 연산 내부에서는 <u>어큐뮬레이터의 값을 증가</u>시킬 수 만 있을 뿐 그 값을 <u>참조해서 사용하는 것은 불가능</u>하다

### 영속화 

p129

Action 1 : numQuestionnaire
Action 2 : totalPoints 
=> 2개의 Job

```scala
scala> val questionnaireRDD = sc.textFile("/csv/questionnaire.csv").map { record =>
     | val splitRecord =record.split(",")
     | val ageRange = splitRecord(0).toInt /10 * 10
     | val maleOrFemale = splitRecord(1)
     | val point=splitRecord(2).toInt
     | (ageRange,maleOrFemale,point)
     | }
questionnaireRDD: org.apache.spark.rdd.RDD[(Int, String, Int)] = MapPartitionsRDD[31] at map at <console>:30

scala> questionnaireRDD.cache
res8: questionnaireRDD.type = MapPartitionsRDD[31] at map at <console>:30

scala> val numQuestionnaire = questionnaireRDD.count
numQuestionnaire: Long = 19

scala> val totalPoints = questionnaire.map(_._3).sum

scala> val totalPoints = questionnaireRDD.map(_._3).sum
totalPoints: Double = 64.0

scala> println(s"AVG ALL:${totalPoints / numQuestionnaire}")
AVG ALL:3.3684210526315788

```

- 1번의 Job으로 계산하기 (2단처리)
  - **reduce 메서드** 
    - 모든 앙케이트에 대해 (평가, 1) 튜플 생성 후 이를 요소로 갖는 RDD 생성 (map)
    - 1에서 생성한 RDD에 포함된 전 요소를 <u>집약처리</u>하고 평가와 1을 각 요소에 더한다 (reduce)

```scala
scala> val (totalPoint, numQuestionnaire) =
     | questionnaireRDD.map(record => (record._3,1)).reduce{
     | case ((intermedPoint, intermedCount),(point,one))=>
     | (intermedPoint + point, intermedCount + one)
     | }
totalPoint: Int = 64
numQuestionnaire: Int = 19

scala> println(s"AVG ALL : ${totalPoints/numQuestionnaire}")
AVG ALL : 3.3684210526315788

```

- 연령대별 평가 평균
  - **reduceByKey**

```scala
scala> val agePointOneRDD= questionnaireRDD.map(record=>(record._1,(record._3,1)))
agePointOneRDD: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[35] at map at <console>:31

scala> val totalPtAndCntPerAgeRDD =agePointOneRDD.reduceByKey{
     | case ((intermedPoint, intermedCount),(point,one))=>
     | (intermedPoint + point, intermedCount+one)
     | }
totalPtAndCntPerAgeRDD: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[36] at reduceByKey at <console>:31

scala> totalPtAndCntPerAgeRDD.collect.foreach {
     | case (ageRange, (totalPoint, count)) =>
     | println(s"AVG Age Range($ageRange): ${totalPoint /count.toDouble}")
     | }
AVG Age Range(30): 3.5
AVG Age Range(50): 1.5
AVG Age Range(40): 2.5
AVG Age Range(20): 3.7142857142857144
AVG Age Range(10): 4.0
```

- **Accumulator**로 성별 단위 평가 평균치 계산

  - 성별 단위로 앙케이트 건수를 세는 어큐뮬레이터

  - 성별 단위로 앙케이트 평가 합계의 어큐뮬레이터

    reduceByKey : 셔플 발생 	VS	 accumulator : 셔플 발생 X 
    	-> <u>개수</u>를 미리 아는 경우에 효과적

```scala
scala> val numAcc =sc.accumulator(0,"Number of M")
warning: there were two deprecation warnings; re-run with -deprecation for details
numAcc: org.apache.spark.Accumulator[Int] = 0

scala> val totalPointMacc=sc.accumulator(0,"Total Points of M")
warning: there were two deprecation warnings; re-run with -deprecation for details
totalPointMacc: org.apache.spark.Accumulator[Int] = 0

scala> val numFAcc = sc.accumulator(0,"Number of F")
warning: there were two deprecation warnings; re-run with -deprecation for details
numFAcc: org.apache.spark.Accumulator[Int] = 0

scala> val totalPointFAcc = sc.accumulator(0,"Total Points of F")
warning: there were two deprecation warnings; re-run with -deprecation for details
totalPointFAcc: org.apache.spark.Accumulator[Int] = 0

scala> questionnaireRDD.foreach {
     | case(_, maleOrFemale, point) => 
     | maleOrFemale match {
     | case "M" =>
     | numAcc +=1
     | totalPointMacc +=point
     | case "F" =>
     | numFAcc += 1
     | totalPointFAcc += point
     | }
     | }

scala> println(s"AVG Male : ${totalPointMacc.value / numAcc.value.toDouble}")
AVG Male : 3.5

scala> println(s"AVG FeMale : ${totalPointFAcc.value / numFAcc.value.toDouble}")
AVG FeMale : 3.272727272727273

```

