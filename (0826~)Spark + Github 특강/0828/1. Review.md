## Spark 

<u>인메모리 기반</u>의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임워크

### spark 구성요소

![spark구성요소](https://user-images.githubusercontent.com/50945713/63816924-03958d00-c975-11e9-9a60-8b26bbecf097.png)

- 클러스터 매니저 : standalone, Yarn, Mesos
- Spark에서는 데이터 처리하기 추상화된 모델 ; RDD (복구 가능한 분산 데이터 셋)

### Spark Application  구현 단계

1. SparkContext 생성
   - Spark 어플리케이션과 Spark 클러스터와의 연결을 담당하는 객체
   - 모든 스파크 어플리케이션은 sparkContext를 이용해 RDD나 accumulator 또는 broadcast 변수 등을 다루게 된다
   - Spark 어플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는 역할
2. RDD(불변 데이터 모델, partition 가능) 생성
   - collection, HDFS, hive, csv  등..
3. RDD 처리 - 변환 연산 (RDD의 요소의 구조 변경, filter처리, grouping.,...)
4. 집계, 요약 처리  - Action\ 연산]
5. 영속화

### Job

- Spark 클러스터 환경에서의 node 들 : SparkClient, Master노드 Worker노드
- SparkClient역할 - SparkApplication  배포 및 실행 요청
- Spark Master노드 역할 - Spark 클러스터 환경에서 사용 가능한 리소스들의 관리
- Spark Worker 노드 역할 - 할당받은 리소스 (CPU core, memory)를 사용해서
- Spark Worker 노드에서 실행되는 프로세스 - Executor는 RDD의 partition 을 Task 단위로 실행



### Spark 장점

- 반복처리와 연속으로 이뤄지는  변환처리 고속화 (메모리 기반)
- 딥러닝, 머신러닝 등의 실행환경에 적합
- 서로 다른 실행환경과 구조, 데이터들의 처리에 대해 통합환경 제공

---

- sc.textFile() 
  : file로부터 RDD 생성

- collect

- map, flatMap()

- mkString("구분자")