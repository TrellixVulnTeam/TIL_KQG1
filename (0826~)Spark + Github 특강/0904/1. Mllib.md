## 머신러닝:MLlib

### 관측과 특성

- 머신러닝에서 특성(feature)는 관측(Observation) 데이터의 속성을 나타내는 용도로 사용됩니다.
- 원본 데이터로부터 특성을 추출하는 과정은 데이터의 변환, 필터링, 정규화, 특성 간 상관 관계 분석 등 다양한 작업을 포함할 수 있습니다.
- 스파크 Mllib 에서는 특성 추출 작업을 더욱 편리하게 수행할 수 있는 다양한 특성 추출 및 변환, 선택 알고리즘과 유틸리티 함수를 제공합니다.

### 지도학습

- 입력에 대한 올바른 출력 값을 알고 있는 데이터셋을 가지고 입력과 그에 따른 출력 값을 함께 학습하게 한 뒤 아직 답이 알려지지 않은 새로운 입력값에 대한 출력값을 찾게 하는 방법
- 훈련을 위해 주어지는 데이터 셋에 각 관측 데이터에 대한 올바른 출력값을 알려주는 레이블(Label)이라는 값이 포함됩니다.
- 스파크에서는 레이블을 포함한 데이터셋을 다루기 위해 LabeledPoint라는 데이터 타입을 사용합니다.

### 연속성 데이터 (Continuous Data)

- 무게나 온도, 습도와 같이 연속적인 값을 가지는 데이터

- 실수값

### 이산형 데이터(Discrete data)

- 나이나 성별, 사과의 개수 등과 같이 불연속적인 값을 가지는 데이터
- 정수나 문자값
- 스파크 MLlib에서 제공하는 API는 연속성 데이터 , 이산형 데이터의 입력 및 출력 데이터 모두 **double 타입**의 데이터만 사용할 수 있습니다.

### 모델

- 알고리즘의 산출물로서 알고리즘에 데이터를 적용해서 만들어낸 결과물.
- 머신러밍의 궁극적인 목적은 입력 데이터로부터 원하는 결과값, 출력 데이터를 얻어내는 것입니다.

### Parametric 방식

- 파라메트릭 알고리즘은 고정된 개수의 파라미터, 즉 **계수**를 사용하는 것으로 입력과 출력 사이의 관계를 특성 값에 관한 수학적 함수 또는 수식으로 가정하고, 이 수식의 결과가 실제 결과값에 가깝도록 계수를 조정하는 방법을 사용합니다.
  - 예) 실제 값과 예측 값 사이의 오차를 나타내는 <u>손실함수(Loss function) 또는 비용함수(Cost Function)</u>를 정의해서 이 함수값을 최소화하는 최적의 기울기와 계수을 찾아내는 방식의 <u>선형회귀</u>나 <u>로지스틱회귀 알고리즘</u>

### Nonparametric 방식

- 입력과 출력 사이의 가설을 세우지 않고 머신러닝의 수행 결과를 그대로 사용하는 방식
- SVM, Naïve Bayes 알고리즘

### 지도학습(Supervised Learning)

- 훈련 데이터에 레이블, 즉 정답에 관한 정보가 포함되며 알고리즘은 입력과 출력에 대한 가설과 정답 정보를 이용해 오차를 계산하고 이를 통해 입력과 출력 사이의 관계를 유추하게 됩니다.
- 회귀(regression)과 분류(classification) 알고리즘
- 훈련용으로 사용하는 모든 데이터에 레이블 정보를 추가해야 합니다.

### 비지도 학습(Unsupervised Learning)

- 특성과 레이블 간의 인과 관계를 모르거나 특별히 지정하지 않고 컴퓨터의 처리에 맡기는 것
- <u>군집(Clustering) 알고리즘</u>

### MLlib API

- spark.mllib 패키지를 사용하는 RDD 기반 API
- spark.ml 패키지는 사용하는 데이터 프레임 기반 API

### MLlib API 기능

- 머신러닝 알고리즘 – classification, regression, clustering, collaborative filtering등 알고리즘 제공
- 특성 추출, 변환, 선택
- 파이프라인 – 여러 종류의 머신러닝 알고리즘을 <u>순차적</u>으로 수행할 수 있는 API제공
- 저장 – 알고리즘 모델, 파이프라인에 대한 저장 및 불러오기 기능을 제공 (xml)
- 유틸리티 – 선형대수, 통계, 데이터 처리 등의 유용한 함수를 제공



### 벡터(Vector)

- 프로그램 상에서 **double 타입**의 값들을 포함하는 컬렉션으로 구현되며 벡터에 포함된 각 데이터는 정의된 순서에 따라 0부터 시작하는 정수형 인덱스를 부여 받습니다.

- org.apache.spark.ml.linalg 패키지에 정의된 트레이트(trait)

- Vector 인스턴스를 만들기 위해서는 값에 대한 정보만 가지고 있는 DenseVector 클래스나 값에 대한 인덱스 정보를 모두 가지고 있는 SparseVector 클래스 중 하나를 선택해서 해당 클래스의 인스턴스를 생성해야 합니다.

- desnse(), sparse() – 팩토리 메서드

- ```scala
  import org.apache.spark.ml.linalg.Vectors
  val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
  val v2 = Vectors.dense(Array(0.1, 0.0, 0.2, 0.3))
  val v3 = Vectors.sparse(4, Seq((0, 0.1), (2, 0.2), (3, 0.3)) //중간에 1이 비어있는건 0.0이 되겠다.
  val v4 = Vectors.sparse(4, Array(0, 2, 3), Array(0.1, 0.2, 0.3))
  
  println(v1.toArray.mkString(", "))
  println(v3.toArray.mkString(", "))
  ```

  =====결과=====

  ```scala
  scala> println(v1.toArray.mkString(", "))
  0.1, 0.0, 0.2, 0.3
  
  scala> println(v3.toArray.mkString(", "))
  0.1, 0.0, 0.2, 0.3
  ```

  

- 



- MLlib은 통계 처리나 머신러닝을 구현하기 위한 라이브러리로, 내부적으로 스파크 코어의 기본 API를 이용하므로 분산처리 기능 자연스럽게 사용 가능.
- 스파크 코어에는 스파크 SQL, MLlib, 스파크 스트리밍, 그래프 X가 있다.

#### 통계 처리, 머신러닝

- 특정 데이터로부터 수학적 기법을 이용하여 그 성질을 끄집어내는 처리
- 과거의 데이터를 이용하여 미래 데이터에 대한 예측을 하는 처리
- 대부분 행렬계산 라이브러리
- 예로는 
  1. 텍스트 정보로부터 긍정적/부정적 표현 여부 판정
  2. 인터넷 쇼핑몰 등에서 사용자의 일정한 패턴을 부석하여 상품 추천
  3. 등등
- [스파크공식사이트](https://spark.apache.org/)에서 최신 정보가 추가 되고 있다.

#### 기본 요소

1. 벡터(Vector)- 1차원 데이터 다루는 데이터 타입

   - LocalVector

     > 기본적인 벡터 타입으로 밀집 벡터와 희소 벡터 클래스를 이용

   - LabeledPoint

     > LocalVector+레이블 
     >
     > 알고리즘이용시 독립변수와 종속 변수 함께 보존시 사용

2. 행렬(Matrix)-1개 이상의 벡터로 구성되는 행렬 형식 데이터 타입

   - LocalMatrix

     > 한 대의 호스트로 행렬 형식 데이터 다루기 위한 데이터 타입

   - DistributedMatrix

     > 대규모 데이터셋을 분산처리하기 위한 행렬 형식 데이터 타입.

#### 분류와 회귀

- 미리 학습해둔 학습 데이터를 통해 분류모델 작성, 따로 처리된 데이터에 모델 을 적용할 수 있다. 

#### 협업 필터링

- 고객/ 상품에 관한 구매 이력 등의 데이터를 입력데이터로 작성하고 이를 이용하여 관련 정보 추천을 한다.

#### 클러스터링

- 데이터가 주어졌을 때 여러 개의 집합으로 나누는 것. 예를 들어 정보성 메일을 받을 때 고객의 특성을 바탕으로 분류하고 이에 적합한 메일을 보내는 것

#### 차원 축소

- 차원 축소 방법으로 SVD,PCA를 이용 할 수 있다. 이를 이용하여 어떤 정보든 형태적으로 유지하면서 데이터 차원을 축소 가능
- 이해하기 쉬운 저차원으로 변환하기 위해, 차원이 너무 높이 알고리즘 분석이 어려운 경우 사용

#### 특징 추출/변환

- 문서 특징 추출하는 방법중 하나로 벡터화 방법으로 TF-IDF, Word2Vec를 사용 가능.
- 수치 데이터 뿐만이 아닌 텏트 데이터도 입력으로 사용가능

#### 빈발 패턴 마이닝

- FP-growth, 연관성 규칙 마이닝, 순차 패턴 마이닝 등을 사용 가능. 이를 이용하여 '빵을 사는 사람은 우유를 사는 경우가 많다' 와 같은 패턴 발견 가능. 

#### PMML 익스포트 기능

#### PMML익스포트 기능

- 스파크 이외의 툴에서 이 모델을 사용할 수 있게 하는 것.

#### K-means 개요

- K-means의 샘플용 데이터는 스파크 본체에 포함

```cmd
cd usr/local/spark
cd data/mllib
cat kmeans_data.txt
```

- 로 확인 가능

```scala
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
val sparkHome=sys.env("SPARK_HOME")
val data=sc.textFile("file://"+sparkHome+"/data/mllib/kmeans_data.txt")
val parsedData=data.map{s=>
Vectors.dense(s.split(' ').map(_.toDouble))}.cache()


val numClusters=2
val numIterations=20
val clusters=KMeans.train(parsedData,numClusters,numIterations)

clusters.k

clusters.clusterCenters

val vec1=Vectors.dense(0.3,0.3,0.3)
clusters.predict(vec1)

val vec2=Vectors.dense(8.0,8.0,8.0)
clusters.predict(vec2)

parsedData.foreach(vec=>
                  println(vec+"=>"+clusters.predict(vec)))

val predictedLabels=parsedData.map(vector=> clusters.predict(vector))
predictedLabels.saveAsTextFile("output/kmeans")
//경로 지정을 안했으므로 하둡에 /user/hadoop/output/kmenas에 있다.
clusters.save(sc, "kmeans_model")


```

```cmd
[hadoop@master ~]$ hadoop fs -cat /user/hadoop/kmeans_model/metadata/part-00000 
{"class":"org.apache.spark.mllib.clustering.KMeansModel","version":"2.0","k":2,"distanceMeasure":"euclidean","trainingCost":0.11999999999994547}
#현재 사용하는 모델의 조건? 등을 알 수 있다.
```



##### 만약 클러스터의 갯수를 모른다면

```scala
val WSSSE=clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors="+WSSSE) Within Set Sum of Squared Errors=0.11999999999994547
```

#### 단어의 벡터화(한국어)

##### 문서에서 단어 추출(형태소 분석)

- 한국어는 단순한 공백으로 분할로는 충분하지 않기에 의미를 갖는 최소한의 단위인 형태소로 분할해야 한다.
- 형태소 분석은 MLlib에 포함되어있지는 않다.
- twitter-korean-text를 이용한다.

[트위터메이븐 한글 자르파일](https://mvnrepository.com/artifact/com.twitter.penguin/korean-text/4.4.4) 다운!

```cmd
$spark_HOME/jars폴더에 mv
```



```scala
libraryDependencies += "com.twitter.penguin" %% "korean_text" % "4.0"
```

```scala
spark-shell --master yarn \
--packages com.twitter.penguin:korean-text:4.4.4

spark-shell \
--master yarn \
--packages com.twitter.penguin:korean-text:4.4.4 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
// 위에 \표시는 엔터 표시임으로 \ 지우고 한줄로 작성해도 된다spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
//안되면 yarn말고 local로 하자
import java.io.StringReader
import org.apache.spark.mllib.feature.{Word2Vec,Word2VecModel}
import com.twitter.penguin.korean.TwitterKoreanProcessor
import com.twitter.penguin.korean.tokenizer.KoreanTokenizer.KoreanToken
import org.apache.spark.mllib.linalg.Vectors

val sentence="이 책은 무슨 책 입니까"
val normalized: CharSequence=TwitterKoreanProcessor.normalize(sentence)

//토크나이즈
val tokens: Seq[KoreanToken]=TwitterKoreanProcessor.tokenize(normalized)

//어근 추출
val stemmed: Seq[KoreanToken]=TwitterKoreanProcessor.stem(tokens)
```





##### 실제 파일로 찾아보자

```scala
val input=sc.textFile("")
```



spark-shell --master local --packages com.twitter.penguin:korean-text:4.4.4 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer

spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.4.4

### 참고

#### 파일 찾기

```cmd
[hadoop@master ~]$ hadoop fs -find / -name kmeans -print
```