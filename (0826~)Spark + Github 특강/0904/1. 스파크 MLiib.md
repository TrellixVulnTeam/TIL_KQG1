## 머신러닝:MLlib

- MLlib은 통계 처리나 머신러닝을 구현하기 위한 라이브러리로, 내부적으로 스파크 코어의 기본 API를 이용하므로 분산처리 기능 자연스럽게 사용 가능.
- 스파크 코어에는 스파크 SQL, MLlib, 스파크 스트리밍, 그래프 X가 있다.

#### 통계 처리, 머신러닝

- 특정 데이터로부터 수학적 기법을 이용하여 그 성질을 끄집어내는 처리
- 과거의 데이터를 이용하여 미래 데이터에 대한 예측을 하는 처리
- 대부분 행렬계산 라이브러리
- 예로는 
  1. 텍스트 정보로부터 긍정적/부정적 표현 여부 판정
  2. 인터넷 쇼핑몰 등에서 사용자의 일정한 패턴을 부석하여 상품 추천
  3. 등등
- [스파크공식사이트](https://spark.apache.org/)에서 최신 정보가 추가 되고 있다.

#### 기본 요소

1. 벡터(Vector)- 1차원 데이터 다루는 데이터 타입

   - LocalVector

     > 기본적인 벡터 타입으로 밀집 벡터와 희소 벡터 클래스를 이용

   - LabeledPoint

     > LocalVector+레이블 
     >
     > 알고리즘이용시 독립변수와 종속 변수 함께 보존시 사용

2. 행렬(Matrix)-1개 이상의 벡터로 구성되는 행렬 형식 데이터 타입

   - LocalMatrix

     > 한 대의 호스트로 행렬 형식 데이터 다루기 위한 데이터 타입

   - DistributedMatrix

     > 대규모 데이터셋을 분산처리하기 위한 행렬 형식 데이터 타입.

#### 분류와 회귀

- 미리 학습해둔 학습 데이터를 통해 분류모델 작성, 따로 처리된 데이터에 모델 을 적용할 수 있다. 

#### 협업 필터링

- 고객/ 상품에 관한 구매 이력 등의 데이터를 입력데이터로 작성하고 이를 이용하여 관련 정보 추천을 한다.

#### 클러스터링

- 데이터가 주어졌을 때 여러 개의 집합으로 나누는 것. 예를 들어 정보성 메일을 받을 때 고객의 특성을 바탕으로 분류하고 이에 적합한 메일을 보내는 것

#### 차원 축소

- 차원 축소 방법으로 SVD,PCA를 이용 할 수 있다. 이를 이용하여 어떤 정보든 형태적으로 유지하면서 데이터 차원을 축소 가능
- 이해하기 쉬운 저차원으로 변환하기 위해, 차원이 너무 높이 알고리즘 분석이 어려운 경우 사용

#### 특징 추출/변환

- 문서 특징 추출하는 방법중 하나로 벡터화 방법으로 TF-IDF, Word2Vec를 사용 가능.
- 수치 데이터 뿐만이 아닌 텏트 데이터도 입력으로 사용가능

#### 빈발 패턴 마이닝

- FP-growth, 연관성 규칙 마이닝, 순차 패턴 마이닝 등을 사용 가능. 이를 이용하여 '빵을 사는 사람은 우유를 사는 경우가 많다' 와 같은 패턴 발견 가능. 

#### PMML 익스포트 기능

#### PMML익스포트 기능

- 스파크 이외의 툴에서 이 모델을 사용할 수 있게 하는 것.

#### K-means 개요

- K-means의 샘플용 데이터는 스파크 본체에 포함

```cmd
cd usr/local/spark
cd data/mllib
cat kmeans_data.txt
```

- 로 확인 가능

```scala
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
val sparkHome=sys.env("SPARK_HOME")
val data=sc.textFile("file://"+sparkHome+"/data/mllib/kmeans_data.txt")
val parsedData=data.map{s=>
Vectors.dense(s.split(' ').map(_.toDouble))}.cache()


val numClusters=2
val numIterations=20
val clusters=KMeans.train(parsedData,numClusters,numIterations)

clusters.k

clusters.clusterCenters

val vec1=Vectors.dense(0.3,0.3,0.3)
clusters.predict(vec1)

val vec2=Vectors.dense(8.0,8.0,8.0)
clusters.predict(vec2)

parsedData.foreach(vec=>
                  println(vec+"=>"+clusters.predict(vec)))

val predictedLabels=parsedData.map(vector=> clusters.predict(vector))
predictedLabels.saveAsTextFile("output/kmeans")
//경로 지정을 안했으므로 하둡에 /user/hadoop/output/kmenas에 있다.
clusters.save(sc, "kmeans_model")


```

```cmd
[hadoop@master ~]$ hadoop fs -cat /user/hadoop/kmeans_model/metadata/part-00000 
{"class":"org.apache.spark.mllib.clustering.KMeansModel","version":"2.0","k":2,"distanceMeasure":"euclidean","trainingCost":0.11999999999994547}
#현재 사용하는 모델의 조건? 등을 알 수 있다.
```



##### 만약 클러스터의 갯수를 모른다면

```scala
val WSSSE=clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors="+WSSSE) Within Set Sum of Squared Errors=0.11999999999994547
```

#### 단어의 벡터화(한국어)

##### 문서에서 단어 추출(형태소 분석)

- 한국어는 단순한 공백으로 분할로는 충분하지 않기에 의미를 갖는 최소한의 단위인 형태소로 분할해야 한다.
- 형태소 분석은 MLlib에 포함되어있지는 않다.
- twitter-korean-text를 이용한다.

[트위터메이븐 한글 자르파일](https://mvnrepository.com/artifact/com.twitter.penguin/korean-text/4.4.4) 다운!

```cmd
$spark_HOME/jars폴더에 mv
```



```scala
libraryDependencies += "com.twitter.penguin" %% "korean_text" % "4.0"
```

```scala
spark-shell --master yarn \
--packages com.twitter.penguin:korean-text:4.4.4

spark-shell \
--master yarn \
--packages com.twitter.penguin:korean-text:4.4.4 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
// 위에 \표시는 엔터 표시임으로 \ 지우고 한줄로 작성해도 된다spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
//안되면 yarn말고 local로 하자
import java.io.StringReader
import org.apache.spark.mllib.feature.{Word2Vec,Word2VecModel}
import com.twitter.penguin.korean.TwitterKoreanProcessor
import com.twitter.penguin.korean.tokenizer.KoreanTokenizer.KoreanToken
import org.apache.spark.mllib.linalg.Vectors

val sentence="이 책은 무슨 책 입니까"
val normalized: CharSequence=TwitterKoreanProcessor.normalize(sentence)

//토크나이즈
val tokens: Seq[KoreanToken]=TwitterKoreanProcessor.tokenize(normalized)

//어근 추출
val stemmed: Seq[KoreanToken]=TwitterKoreanProcessor.stem(tokens)
```





##### 실제 파일로 찾아보자

```scala
val input=sc.textFile("")
```



spark-shell --master local --packages com.twitter.penguin:korean-text:4.4.4 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer

spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.4.4

### 참고

#### 파일 찾기

```cmd
[hadoop@master ~]$ hadoop fs -find / -name kmeans -print
```