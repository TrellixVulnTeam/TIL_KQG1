### Labeled Point

- 레이블(Label)을 사용하는 경우를 위한 벡터로서 특성 값들을 담고 있는 벡터와 레이블 정보로 구성됩니다.

- 레이블에는 double 타입의 값만 할당할 수 있으며

- 로지스틱 회귀와 같은 이진 분류 알고리즘을 사용할 경우 0(negative) 또는 1(positive)로 설정합니다.

- ```scala
  scala> import org.apache.spark.ml.feature.LabeledPoint
  import org.apache.spark.ml.feature.LabeledPoint
  
  scala> import org.apache.spark.ml.linalg.Vectors
  import org.apache.spark.ml.linalg.Vectors
  
  scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
  v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]
  
  scala> val v5 = LabeledPoint(1.0, v1)
  v5: org.apache.spark.ml.feature.LabeledPoint = (1.0,[0.1,0.0,0.2,0.3])
  
  scala> println(s"label:${v5.label}, features:${v5.features}")
  label:1.0, features:[0.1,0.0,0.2,0.3]
  
  
  ```

  

---

### Pipeline 

- 머신러닝은 데이터 수집부터, 가공, 특성 추출, 알고리즘 적용 및 모델 생성, 평가, 배포 및 활용에 이르는 일련의 작업을 반복하며 수행됩니다.

- 파이프라인은 여러 종류의 알고리즘을 순차적으로 실행할 수 있게 지원하는 고차원 API이며, 파이프 라인 API를 이용해 머신러닝을 위한 워크 플로우를 생성할 수 있습니다.

- 파이프라인은 데이터 프레임을 사용합니다.

- transformer – org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임을 변형해 새로운 데이터프레임을 생성하는 용도로 사용

- Estimator - org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머를 생성하는 역할을 합니다.

- Pipeline - org.apache.spark.ml 패키지에 선언된 클래스. 여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자. 하나의 파이프라인은 여러 개의 파이프라인 스테이지(PipelineStage)로 구성되며, 등록된 파이프라인 스테이지들은 우선순위에 따라 순차적으로 실행됩니다.

- ParamMap : 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스

  ```scala
  scala> import org.apache.spark.ml.feature.LabeledPoint
  import org.apache.spark.ml.feature.LabeledPoint
  
  scala> import org.apache.spark.ml.linalg.Vectors
  import org.apache.spark.ml.linalg.Vectors
  
  scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
  v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]
  
  scala> val v5 = LabeledPoint(1.0, v1)
  v5: org.apache.spark.ml.feature.LabeledPoint = (1.0,[0.1,0.0,0.2,0.3])
  
  scala> println(s"label:${v5.label}, features:${v5.features}")
  label:1.0, features:[0.1,0.0,0.2,0.3]
  
  scala> val training = spark.createDataFrame(Seq((161.0, 69.87, 29, 1.0),(176.78, 74.35, 34, 1.0),(159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender")
  training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 2 more fields]
  
  scala> training.cache()
  res2: training.type = [height: double, weight: double ... 2 more fields]
  
  scala> val test = spark.createDataFrame(Seq((169.4, 75.3, 42),(185.1, 85.0, 37),(161.6, 61.2, 28))).toDF("height", "weight", "age")
  test: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 1 more field]
  
  scala> training.show(false)
  +------+------+---+------+
  |height|weight|age|gender|
  +------+------+---+------+
  |161.0 |69.87 |29 |1.0   |
  |176.78|74.35 |34 |1.0   |
  |159.23|58.32 |29 |0.0   |
  +------+------+---+------+
  
  scala>  import org.apache.spark.ml.feature.VectorAssembler
  import org.apache.spark.ml.feature.VectorAssembler
  
  scala> val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")
  assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7f26e6077190
  
  scala> val assembled_training = assembler.transform(training)
  assembled_training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 3 more fields]
  
  scala> assemble
  assembled_training   assembler
  
  scala> assembled_training.show(false)
  +------+------+---+------+-------------------+
  |height|weight|age|gender|features           |
  +------+------+---+------+-------------------+
  |161.0 |69.87 |29 |1.0   |[161.0,69.87,29.0] |
  |176.78|74.35 |34 |1.0   |[176.78,74.35,34.0]|
  |159.23|58.32 |29 |0.0   |[159.23,58.32,29.0]|
  +------+------+---+------+-------------------+
  
  
  scala>  import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
  import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
  
  scala> val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01).setLabelCol("gender")
  lr: org.apache.spark.ml.classification.LogisticRegression = logreg_db1a00187036
  
  scala> val model = lr.fit(assembled_training)
  19/09/05 18:40:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  19/09/05 18:40:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_db1a00187036, numClasses = 2, numFeatures = 3
  
  scala> model.transform(assembled_training).show()    
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  |height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  | 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
  |176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
  |159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  
  
  scala>  import org.apache.spark.ml.{Pipeline, PipelineModel}
  import org.apache.spark.ml.{Pipeline, PipelineModel}
  
  scala> val pipeline = new Pipeline().setStages(Array(assembler, lr))    
  pipeline: org.apache.spark.ml.Pipeline = pipeline_316c5e6fa0de
  
  scala> val pipelineModel = pipeline.fit(training)    
  pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_316c5e6fa0de
  
  scala> pipelineModel.transform(training).show()    
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  |height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  | 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
  |176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
  |159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  
  scala> val path1 = "/output/sparkmllib/regression-model"    
  path1: String = /output/sparkmllib/regression-model
  
  scala> val path2 = "/output/sparkmllib/pipelinemodel"    
  path2: String = /output/sparkmllib/pipelinemodel
  
  scala> model.write.overwrite().save(path1)    
                                                                                  
  scala> pipelineModel.write.overwrite().save(path2)    
  
  scala> val loadedModel = LogisticRegressionModel.load(path1)    
  loadedModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_db1a00187036, numClasses = 2, numFeatures = 3
  
  scala> val loadedPipelineModel = PipelineModel.load(path2)    
  loadedPipelineModel: org.apache.spark.ml.PipelineModel = pipeline_316c5e6fa0de
  
  scala> spark.stop
  ```

  

### 알고리즘

#### Tokenizer

: 공백 문자를 기준으로 입력 문자열을 개별 단어의 배열로 변환하고 이 <u>배열</u>을 값으로 하는 <u>새로운 컬럼을 생성</u>하는 트랜스포머. <u>문자열을 기반으로 하는 특성 처리에 자주 사용</u>됨

#### RegexTokenizer 

: 정규식을 사용하여 문자열을 기반으로 하는 특성 처리  

```scala
cala> import org.apache.spark.ml.feature.Tokenizer
import org.apache.spark.ml.feature.Tokenizer

scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

//원소하나하나마다 tuple적용
scala> val data = Seq("Tokenization is the process", "Refer to the Tokenizer").map(Tuple1(_))    
data: Seq[(String,)] = List((Tokenization is the process,), (Refer to the Tokenizer,))

scala> val inputDF = spark.createDataFrame(data).toDF("input")    
inputDF: org.apache.spark.sql.DataFrame = [input: string]

scala> val tokenizer = new Tokenizer().setInputCol("input").setOutputCol("output")   
tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_bddb8816d6d8

scala> val outputDF = tokenizer.transform(inputDF)    
outputDF: org.apache.spark.sql.DataFrame = [input: string, output: array<string>]

scala> outputDF.printSchema()    
root
 |-- input: string (nullable = true)
 |-- output: array (nullable = true)
 |    |-- element: string (containsNull = true)


scala> outputDF.show(false)
+---------------------------+--------------------------------+
|input                      |output                          |
+---------------------------+--------------------------------+
|Tokenization is the process|[tokenization, is, the, process]|
|Refer to the Tokenizer     |[refer, to, the, tokenizer]     |
+---------------------------+--------------------------------+


```

#### TF-IDF (Term Frequency – Inverse Document Frequency)

#### StringIndexer

: 문자열 컬럼에 대응하는 숫자형 컬럼을 생성하는 평가자. 

- StringIndexer는 문자열 레이블 컬럼에 적용하며 해당 컬럼의 모든 문자열에 <u>노출 빈도</u>에 따른 <u>인덱스</u>를 부여해서 <u>숫자로 된 새로운 레이블 컬럼</u>을 생성합니다.

- StringIndexer는 트랜스포머(변환)가 아닌 평가자 (Estimator: 알고리즘을 생성한 테스트 데이터로부터 나온 모델 생성)로서 **fit()** 메서드를 이용해 stringIndexerModel을 생성하며 이 모델을 이용해 문자열 인코딩을 수행할 수 있습니다. 

- ```scala
  cala> val spark = SparkSession .builder() .appName("StringIndexerSample") .master("local[*]") .getOrCreate() 
  spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6832558c
  
  scala> val df1 = spark.createDataFrame(Seq(      (0, "red"),      (1, "blue"),      (2, "green"),      (3, "yellow"))).toDF("id", "color")   
  df1: org.apache.spark.sql.DataFrame = [id: int, color: string]
  
  scala> val strignIndexer = new StringIndexer().setInputCol("color") .setOutputCol("colorIndex") .fit(df1)    
  strignIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_670b0e59a9e3
  
  scala> val df2 = strignIndexer.transform(df1)    
  df2: org.apache.spark.sql.DataFrame = [id: int, color: string ... 1 more field]
  
  scala> df2.show(false)    
  +---+------+----------+
  |id |color |colorIndex|
  +---+------+----------+
  |0  |red   |3.0       |
  |1  |blue  |0.0       |
  |2  |green |2.0       |
  |3  |yellow|1.0       |
  +---+------+----------+
  
  
  scala> val indexToString = new IndexToString() .setInputCol("colorIndex") .setOutputCol("originalColor") 
  indexToString: org.apache.spark.ml.feature.IndexToString = idxToStr_5e82124afe2c
  
  scala> val df3 = indexToString.transform(df2)   
  df3: org.apache.spark.sql.DataFrame = [id: int, color: string ... 2 more fields]
  
  scala> df3.show(false)    
  +---+------+----------+-------------+
  |id |color |colorIndex|originalColor|
  +---+------+----------+-------------+
  |0  |red   |3.0       |red          |
  |1  |blue  |0.0       |blue         |
  |2  |green |2.0       |green        |
  |3  |yellow|1.0       |yellow       |
  +---+------+----------+-------------+
  
  
  ```

### 데이터 전처리 (p275)

1. case class

```scala
scala> case class Weather( date: String,
     |                     day_of_week: String,
     |                     avg_temp: Double,
     |                     max_temp: Double,
     |                     min_temp: Double,
     |                     rainfall: Double,
     |                     daylight_hours: Double,
     |                     max_depth_snowfall: Double,
     |                     total_snowfall: Double,
     |                     solar_radiation: Double,
     |                     mean_wind_speed: Double,
     |                     max_wind_speed: Double,
     |                     max_instantaneous_wind_speed: Double,
     |                     avg_humidity: Double,
     |                     avg_cloud_cover: Double)
defined class Weather

scala> case class Sales(date: String, sales: Double)
defined class Sales

scala> import spark.implicits._
import spark.implicits._

scala> import org.apache.spark.mllib.regression.{LabeledPoint,LinearRegressionWithSGD}
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> import org.apache.spark.mllib.feature.StandardScaler
import org.apache.spark.mllib.feature.StandardScaler

scala> import org.apache.spark.mllib.evaluation.RegressionMetrics
import org.apache.spark.mllib.evaluation.RegressionMetrics

scala> import org.apache.spark.sql.functions.udf
import org.apache.spark.sql.functions.udf

scala> val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
weatherCSVRDD: org.apache.spark.rdd.RDD[String] = /data/sales/weather.csv MapPartitionsRDD[1] at textFile at <console>:32

scala> val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
headerOfWeatherCSVRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:34

scala> val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)
weatherCSVwithoutHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at subtract at <console>:35

scala> val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
     |       map(p => Weather(p(0),
     |       p(1),
     |       p(2).trim.toDouble,
     |       p(3).trim.toDouble,
     |       p(4).trim.toDouble,
     |       p(5).trim.toDouble,
     |       p(6).trim.toDouble,
     |       p(7).trim.toDouble,
     |       p(8).trim.toDouble,
     |       p(9).trim.toDouble,
     |       p(10).trim.toDouble,
     |       p(11).trim.toDouble,
     |       p(12).trim.toDouble,
     |       p(13).trim.toDouble,
     |       p(14).trim.toDouble
     |     )).toDF()
weatherDF: org.apache.spark.sql.DataFrame = [date: string, day_of_week: string ... 13 more fields]

scala> val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
salesCSVRDD: org.apache.spark.rdd.RDD[String] = /data/sales/sales.csv MapPartitionsRDD[10] at textFile at <console>:32

scala> val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
headerOfSalesCSVRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[11] at parallelize at <console>:34

scala> val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
salesCSVwithoutHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at subtract at <console>:35

scala> val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()
salesDF: org.apache.spark.sql.DataFrame = [date: string, sales: double]

scala> println(weatherDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- day_of_week: string (nullable = true)
 |-- avg_temp: double (nullable = false)
 |-- max_temp: double (nullable = false)
 |-- min_temp: double (nullable = false)
 |-- rainfall: double (nullable = false)
 |-- daylight_hours: double (nullable = false)
 |-- max_depth_snowfall: double (nullable = false)
 |-- total_snowfall: double (nullable = false)
 |-- solar_radiation: double (nullable = false)
 |-- mean_wind_speed: double (nullable = false)
 |-- max_wind_speed: double (nullable = false)
 |-- max_instantaneous_wind_speed: double (nullable = false)
 |-- avg_humidity: double (nullable = false)
 |-- avg_cloud_cover: double (nullable = false)
()

scala> println(salesDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- sales: double (nullable = false)
()



```



2. 데이터의 전처리(날짜 기준으로 조인 후, 요일 컬럼값을 수치화하고, 요일컬럼제거후 , 수치화된 주말컬럼 추가)

   - ```scala
     scala> val salesAndWeatherDF = salesDF.join(weatherDF, "date")
     salesAndWeatherDF: org.apache.spark.sql.DataFrame = [date: string, sales: double ... 14 more fields]
     
     scala> val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d 
          |                                        else 0d)
     isWeekend: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))
     
     scala> val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")
     replacedSalesAndWeatherDF: org.apache.spark.sql.DataFrame = [date: string, sales: double ... 14 more fields]
     ```

     

3. 매출에 영향을 주는 독립변수만 추출하여 새로운 데이터 프레임 생성
   매출에 영향을 주는 독립변수 평균기온, 일강수량, 휴일을 선택

   - ```scala
     scala> val selectedDataDF = replacedSalesAndWeatherDF.select("sales","avg_temp","rainfall","weekend")
     selectedDataDF: org.apache.spark.sql.DataFrame = [sales: double, avg_temp: double ... 2 more fields]
     ```

     

4. 데이터프레임을 회귀분석을 위한 Vector, LabeledPoint로 생성

   - ```scala
     scala>  val labeledPointsRDD = selectedDataDF.rdd.map(row => LabeledPoint(row.getDouble(0),
          |  Vectors.dense(
              row.getDouble(1),
              row.getDouble(2),
              row.getDouble(3))))
     labeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[39] at map at <console>:33
     ```
     
     

5. 데이터 특성을 표준화 (평균 0, 분산1 인 스케일러 사용)

   - ```scala
     scala> val scaler = new StandardScaler().fit(labeledPointsRDD.map(x =>x.features))
     scaler: org.apache.spark.mllib.feature.StandardScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@13e16a55
     
     scala> val scaledLabledPointsRDD = labeledPointsRDD.map(x => LabeledPoint(x.label, scaler.transform(x.features)))
     scaledLabledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[45] at map at <console>:35
     ```

6. 선형 회귀모델 작성

   - ```scala
     scala> val numIterations = 20
     numIterations: Int = 20
     
     scala>     scaledLabledPointsRDD.cache
     res0: scaledLabledPointsRDD.type = MapPartitionsRDD[45] at map at <console>:35
     
     scala>     val linearRegressionModel = LinearRegressionWithSGD.train(scaledLabledPointsRDD, numIterations)
     warning: there was one deprecation warning; re-run with -deprecation for details
     19/09/05 22:37:23 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
     19/09/05 22:37:23 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
     linearRegressionModel: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 3
     
     scala>     println("weights :" + linearRegressionModel.weights)
     weights :[199554.8706936455,30004.300595788493,158369.3623306316]
     
     ```

7. 알고리즘에 미지의 데이터 적용해 예측

   - ```scala
     scala> val targetDataVector1 = Vectors.dense(15.0,15.4,1)
     targetDataVector1: org.apache.spark.mllib.linalg.Vector = [15.0,15.4,1.0]
     
     scala>     val targetDataVector2 = Vectors.dense(20.0,0,0)
     targetDataVector2: org.apache.spark.mllib.linalg.Vector = [20.0,0.0,0.0]
     
     scala>     val targetScaledDataVector1 = scaler.transform(targetDataVector1)
     targetScaledDataVector1: org.apache.spark.mllib.linalg.Vector = [1.6692119810227204,1.4033030983491395,2.208721944576236]
     
     scala>     val targetScaledDataVector2 = scaler.transform(targetDataVector2)
     targetScaledDataVector2: org.apache.spark.mllib.linalg.Vector = [2.225615974696961,0.0,0.0]
     
     scala>     val result1 = linearRegressionModel.predict(targetScaledDataVector1)
     result1: Double = 724998.3949513528
     
     scala>     val result2 = linearRegressionModel.predict(targetScaledDataVector2)
     result2: Double = 444132.5080443638
     
     scala>     println("avg_tmp=15.0,rainfall=15.4,weekend=true : sales = " + result1)
     avg_tmp=15.0,rainfall=15.4,weekend=true : sales = 724998.3949513528
     
     scala>     println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)
     avg_tmp=20.0,rainfall=0,weekend=false : sales = 444132.5080443638
     
     ```

8. 입력데이터 분할 및 평가

   - ```scala
     scala> val splitScaledLabeledPointsRDD = scaledLabledPointsRDD.randomSplit(Array(0.6, 0.4), seed = 11L)
     splitScaledLabeledPointsRDD: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[108] at randomSplit at <console>:33, MapPartitionsRDD[109] at randomSplit at <console>:33)
     
     scala>     val trainingScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(0).cache()
     trainingScaledLabeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[108] at randomSplit at <console>:33
     
     scala>     val testScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(1)
     testScaledLabeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[109] at randomSplit at <console>:33
     
     scala>     val linearRegressionModel2 = LinearRegressionWithSGD.train(trainingScaledLabeledPointsRDD, numIterations)
     warning: there was one deprecation warning; re-run with -deprecation for details
     linearRegressionModel2: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 3
     
     scala>     val scoreAndLabels = testScaledLabeledPointsRDD.map { point =>
          |      val score = linearRegressionModel2.predict(point.features)
          |       (score, point.label)
          |     }
     scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[172] at map at <console>:35
     
     scala> val metrics = new RegressionMetrics(scoreAndLabels)
     metrics: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@174efba5
     
     scala>     println("RMSE = "+ metrics.rootMeanSquaredError)
     RMSE = 299353.80381584406                                                       
     ```

9. 작성한 모델 보존

   - ```scala
     linearRegressionModel.save(sc, "/output/mllib/model/")
     
     import org.apache.spark.mllib.regression.LinearRegressionModel
     val model2 = LinearRegressionModel.load(sc, "/output/mllib/model/")
     ```

10. 스파크를 실행시킨 디렉토리 경로 아래에 파일 생성

    - ```scala
      scala> linearRegressionModel.toPMML("model.pmml")
      ```

11. cat으로 확인!

    - ```scala
      [hadoop@master ~]$ cat model.pmml
      <?xml version="1.0" encoding="UTF-8" standalone="yes"?>
      <PMML xmlns="http://www.dmg.org/PMML-4_2" version="4.2">
          <Header description="linear regression">
              <Application name="Apache Spark MLlib" version="2.4.3"/>
              <Timestamp>2019-09-05T22:58:00</Timestamp>
          </Header>
          <DataDictionary numberOfFields="4">
              <DataField name="field_0" optype="continuous" dataType="double"/>
              <DataField name="field_1" optype="continuous" dataType="double"/>
              <DataField name="field_2" optype="continuous" dataType="double"/>
              <DataField name="target" optype="continuous" dataType="double"/>
          </DataDictionary>
          <RegressionModel modelName="linear regression" functionName="regression">
              <MiningSchema>
                  <MiningField name="field_0" usageType="active"/>
                  <MiningField name="field_1" usageType="active"/>
                  <MiningField name="field_2" usageType="active"/>
                  <MiningField name="target" usageType="target"/>
              </MiningSchema>
              <RegressionTable intercept="0.0">
                  <NumericPredictor name="field_0" coefficient="199554.8706936455"/>
                  <NumericPredictor name="field_1" coefficient="30004.300595788493"/>
                  <NumericPredictor name="field_2" coefficient="158369.3623306316"/>
              </RegressionTable>
          </RegressionModel>
      </PMML>
      
      ```

      