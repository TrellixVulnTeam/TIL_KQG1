### Labeled Point

- 레이블(Label)을 사용하는 경우를 위한 벡터로서 특성 값들을 담고 있는 벡터와 레이블 정보로 구성됩니다.

- 레이블에는 double 타입의 값만 할당할 수 있으며

- 로지스틱 회귀와 같은 이진 분류 알고리즘을 사용할 경우 0(negative) 또는 1(positive)로 설정합니다.

- ```scala
  scala> import org.apache.spark.ml.feature.LabeledPoint
  import org.apache.spark.ml.feature.LabeledPoint
  
  scala> import org.apache.spark.ml.linalg.Vectors
  import org.apache.spark.ml.linalg.Vectors
  
  scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
  v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]
  
  scala> val v5 = LabeledPoint(1.0, v1)
  v5: org.apache.spark.ml.feature.LabeledPoint = (1.0,[0.1,0.0,0.2,0.3])
  
  scala> println(s"label:${v5.label}, features:${v5.features}")
  label:1.0, features:[0.1,0.0,0.2,0.3]
  
  
  ```

  

---

### Pipeline 

- 머신러닝은 데이터 수집부터, 가공, 특성 추출, 알고리즘 적용 및 모델 생성, 평가, 배포 및 활용에 이르는 일련의 작업을 반복하며 수행됩니다.

- 파이프라인은 여러 종류의 알고리즘을 순차적으로 실행할 수 있게 지원하는 고차원 API이며, 파이프 라인 API를 이용해 머신러닝을 위한 워크 플로우를 생성할 수 있습니다.

- 파이프라인은 데이터 프레임을 사용합니다.

- transformer – org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임을 변형해 새로운 데이터프레임을 생성하는 용도로 사용

- Estimator - org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머를 생성하는 역할을 합니다.

- Pipeline - org.apache.spark.ml 패키지에 선언된 클래스. 여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자. 하나의 파이프라인은 여러 개의 파이프라인 스테이지(PipelineStage)로 구성되며, 등록된 파이프라인 스테이지들은 우선순위에 따라 순차적으로 실행됩니다.

- ParamMap : 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스

  ```scala
  scala> import org.apache.spark.ml.feature.LabeledPoint
  import org.apache.spark.ml.feature.LabeledPoint
  
  scala> import org.apache.spark.ml.linalg.Vectors
  import org.apache.spark.ml.linalg.Vectors
  
  scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
  v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]
  
  scala> val v5 = LabeledPoint(1.0, v1)
  v5: org.apache.spark.ml.feature.LabeledPoint = (1.0,[0.1,0.0,0.2,0.3])
  
  scala> println(s"label:${v5.label}, features:${v5.features}")
  label:1.0, features:[0.1,0.0,0.2,0.3]
  
  scala> val training = spark.createDataFrame(Seq((161.0, 69.87, 29, 1.0),(176.78, 74.35, 34, 1.0),(159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender")
  training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 2 more fields]
  
  scala> training.cache()
  res2: training.type = [height: double, weight: double ... 2 more fields]
  
  scala> val test = spark.createDataFrame(Seq((169.4, 75.3, 42),(185.1, 85.0, 37),(161.6, 61.2, 28))).toDF("height", "weight", "age")
  test: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 1 more field]
  
  scala> training.show(false)
  +------+------+---+------+
  |height|weight|age|gender|
  +------+------+---+------+
  |161.0 |69.87 |29 |1.0   |
  |176.78|74.35 |34 |1.0   |
  |159.23|58.32 |29 |0.0   |
  +------+------+---+------+
  
  scala>  import org.apache.spark.ml.feature.VectorAssembler
  import org.apache.spark.ml.feature.VectorAssembler
  
  scala> val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")
  assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7f26e6077190
  
  scala> val assembled_training = assembler.transform(training)
  assembled_training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 3 more fields]
  
  scala> assemble
  assembled_training   assembler
  
  scala> assembled_training.show(false)
  +------+------+---+------+-------------------+
  |height|weight|age|gender|features           |
  +------+------+---+------+-------------------+
  |161.0 |69.87 |29 |1.0   |[161.0,69.87,29.0] |
  |176.78|74.35 |34 |1.0   |[176.78,74.35,34.0]|
  |159.23|58.32 |29 |0.0   |[159.23,58.32,29.0]|
  +------+------+---+------+-------------------+
  
  
  scala>  import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
  import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
  
  scala> val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01).setLabelCol("gender")
  lr: org.apache.spark.ml.classification.LogisticRegression = logreg_db1a00187036
  
  scala> val model = lr.fit(assembled_training)
  19/09/05 18:40:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  19/09/05 18:40:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_db1a00187036, numClasses = 2, numFeatures = 3
  
  scala> model.transform(assembled_training).show()    
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  |height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  | 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
  |176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
  |159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  
  
  scala>  import org.apache.spark.ml.{Pipeline, PipelineModel}
  import org.apache.spark.ml.{Pipeline, PipelineModel}
  
  scala> val pipeline = new Pipeline().setStages(Array(assembler, lr))    
  pipeline: org.apache.spark.ml.Pipeline = pipeline_316c5e6fa0de
  
  scala> val pipelineModel = pipeline.fit(training)    
  pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_316c5e6fa0de
  
  scala> pipelineModel.transform(training).show()    
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  |height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  | 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
  |176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
  |159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
  +------+------+---+------+-------------------+--------------------+--------------------+----------+
  
  scala> val path1 = "/output/sparkmllib/regression-model"    
  path1: String = /output/sparkmllib/regression-model
  
  scala> val path2 = "/output/sparkmllib/pipelinemodel"    
  path2: String = /output/sparkmllib/pipelinemodel
  
  scala> model.write.overwrite().save(path1)    
                                                                                  
  scala> pipelineModel.write.overwrite().save(path2)    
  
  scala> val loadedModel = LogisticRegressionModel.load(path1)    
  loadedModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_db1a00187036, numClasses = 2, numFeatures = 3
  
  scala> val loadedPipelineModel = PipelineModel.load(path2)    
  loadedPipelineModel: org.apache.spark.ml.PipelineModel = pipeline_316c5e6fa0de
  
  scala> spark.stop
  ```

  

### 알고리즘

#### Tokenizer

: 공백 문자를 기준으로 입력 문자열을 개별 단어의 배열로 변환하고 이 <u>배열</u>을 값으로 하는 <u>새로운 컬럼을 생성</u>하는 트랜스포머. <u>문자열을 기반으로 하는 특성 처리에 자주 사용</u>됨

#### RegexTokenizer 

: 정규식을 사용하여 문자열을 기반으로 하는 특성 처리  

```scala
cala> import org.apache.spark.ml.feature.Tokenizer
import org.apache.spark.ml.feature.Tokenizer

scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

//원소하나하나마다 tuple적용
scala> val data = Seq("Tokenization is the process", "Refer to the Tokenizer").map(Tuple1(_))    
data: Seq[(String,)] = List((Tokenization is the process,), (Refer to the Tokenizer,))

scala> val inputDF = spark.createDataFrame(data).toDF("input")    
inputDF: org.apache.spark.sql.DataFrame = [input: string]

scala> val tokenizer = new Tokenizer().setInputCol("input").setOutputCol("output")   
tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_bddb8816d6d8

scala> val outputDF = tokenizer.transform(inputDF)    
outputDF: org.apache.spark.sql.DataFrame = [input: string, output: array<string>]

scala> outputDF.printSchema()    
root
 |-- input: string (nullable = true)
 |-- output: array (nullable = true)
 |    |-- element: string (containsNull = true)


scala> outputDF.show(false)
+---------------------------+--------------------------------+
|input                      |output                          |
+---------------------------+--------------------------------+
|Tokenization is the process|[tokenization, is, the, process]|
|Refer to the Tokenizer     |[refer, to, the, tokenizer]     |
+---------------------------+--------------------------------+


```

#### TF-IDF (Term Frequency – Inverse Document Frequency)

#### StringIndexer

: 문자열 컬럼에 대응하는 숫자형 컬럼을 생성하는 평가자. 

- StringIndexer는 문자열 레이블 컬럼에 적용하며 해당 컬럼의 모든 문자열에 <u>노출 빈도</u>에 따른 <u>인덱스</u>를 부여해서 <u>숫자로 된 새로운 레이블 컬럼</u>을 생성합니다.

- StringIndexer는 트랜스포머(변환)가 아닌 평가자 (Estimator: 알고리즘을 생성한 테스트 데이터로부터 나온 모델 생성)로서 **fit()** 메서드를 이용해 stringIndexerModel을 생성하며 이 모델을 이용해 문자열 인코딩을 수행할 수 있습니다. 

- ```scala
  cala> val spark = SparkSession .builder() .appName("StringIndexerSample") .master("local[*]") .getOrCreate() 
  spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6832558c
  
  scala> val df1 = spark.createDataFrame(Seq(      (0, "red"),      (1, "blue"),      (2, "green"),      (3, "yellow"))).toDF("id", "color")   
  df1: org.apache.spark.sql.DataFrame = [id: int, color: string]
  
  scala> val strignIndexer = new StringIndexer().setInputCol("color") .setOutputCol("colorIndex") .fit(df1)    
  strignIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_670b0e59a9e3
  
  scala> val df2 = strignIndexer.transform(df1)    
  df2: org.apache.spark.sql.DataFrame = [id: int, color: string ... 1 more field]
  
  scala> df2.show(false)    
  +---+------+----------+
  |id |color |colorIndex|
  +---+------+----------+
  |0  |red   |3.0       |
  |1  |blue  |0.0       |
  |2  |green |2.0       |
  |3  |yellow|1.0       |
  +---+------+----------+
  
  
  scala> val indexToString = new IndexToString() .setInputCol("colorIndex") .setOutputCol("originalColor") 
  indexToString: org.apache.spark.ml.feature.IndexToString = idxToStr_5e82124afe2c
  
  scala> val df3 = indexToString.transform(df2)   
  df3: org.apache.spark.sql.DataFrame = [id: int, color: string ... 2 more fields]
  
  scala> df3.show(false)    
  +---+------+----------+-------------+
  |id |color |colorIndex|originalColor|
  +---+------+----------+-------------+
  |0  |red   |3.0       |red          |
  |1  |blue  |0.0       |blue         |
  |2  |green |2.0       |green        |
  |3  |yellow|1.0       |yellow       |
  +---+------+----------+-------------+
  
  
  ```

### 데이터 전처리 (p275)

```scala
scala> case class Weather( date: String,
     |                     day_of_week: String,
     |                     avg_temp: Double,
     |                     max_temp: Double,
     |                     min_temp: Double,
     |                     rainfall: Double,
     |                     daylight_hours: Double,
     |                     max_depth_snowfall: Double,
     |                     total_snowfall: Double,
     |                     solar_radiation: Double,
     |                     mean_wind_speed: Double,
     |                     max_wind_speed: Double,
     |                     max_instantaneous_wind_speed: Double,
     |                     avg_humidity: Double,
     |                     avg_cloud_cover: Double)
defined class Weather

scala> case class Sales(date: String, sales: Double)
defined class Sales

scala> import spark.implicits._
import spark.implicits._

scala> import org.apache.spark.mllib.regression.{LabeledPoint,LinearRegressionWithSGD}
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> import org.apache.spark.mllib.feature.StandardScaler
import org.apache.spark.mllib.feature.StandardScaler

scala> import org.apache.spark.mllib.evaluation.RegressionMetrics
import org.apache.spark.mllib.evaluation.RegressionMetrics

scala> import org.apache.spark.sql.functions.udf
import org.apache.spark.sql.functions.udf

scala> val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
weatherCSVRDD: org.apache.spark.rdd.RDD[String] = /data/sales/weather.csv MapPartitionsRDD[1] at textFile at <console>:32

scala> val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
headerOfWeatherCSVRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:34

scala> val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)
weatherCSVwithoutHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at subtract at <console>:35

scala> val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
     |       map(p => Weather(p(0),
     |       p(1),
     |       p(2).trim.toDouble,
     |       p(3).trim.toDouble,
     |       p(4).trim.toDouble,
     |       p(5).trim.toDouble,
     |       p(6).trim.toDouble,
     |       p(7).trim.toDouble,
     |       p(8).trim.toDouble,
     |       p(9).trim.toDouble,
     |       p(10).trim.toDouble,
     |       p(11).trim.toDouble,
     |       p(12).trim.toDouble,
     |       p(13).trim.toDouble,
     |       p(14).trim.toDouble
     |     )).toDF()
weatherDF: org.apache.spark.sql.DataFrame = [date: string, day_of_week: string ... 13 more fields]

scala> val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
salesCSVRDD: org.apache.spark.rdd.RDD[String] = /data/sales/sales.csv MapPartitionsRDD[10] at textFile at <console>:32

scala> val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
headerOfSalesCSVRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[11] at parallelize at <console>:34

scala> val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
salesCSVwithoutHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at subtract at <console>:35

scala> val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()
salesDF: org.apache.spark.sql.DataFrame = [date: string, sales: double]

scala> println(weatherDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- day_of_week: string (nullable = true)
 |-- avg_temp: double (nullable = false)
 |-- max_temp: double (nullable = false)
 |-- min_temp: double (nullable = false)
 |-- rainfall: double (nullable = false)
 |-- daylight_hours: double (nullable = false)
 |-- max_depth_snowfall: double (nullable = false)
 |-- total_snowfall: double (nullable = false)
 |-- solar_radiation: double (nullable = false)
 |-- mean_wind_speed: double (nullable = false)
 |-- max_wind_speed: double (nullable = false)
 |-- max_instantaneous_wind_speed: double (nullable = false)
 |-- avg_humidity: double (nullable = false)
 |-- avg_cloud_cover: double (nullable = false)
()

scala> println(salesDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- sales: double (nullable = false)
()



```



- 데이터의 전처리(날짜 기준으로 조인 후, 요일 컬럼값을 수치화하고, 요일컬럼제거후 , 수치화된 주말컬럼 추가)