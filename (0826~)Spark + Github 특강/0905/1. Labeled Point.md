### Labeled Point

---

Pipeline 

§파이프 라인

•머신러닝은 데이터 수집부터, 가공, 특성 추출, 알고리즘 적용 및 모델 생성, 평가, 배포 및 활용에 이르는 일련의 작업을 반복하며 수행됩니다.

•파이프라인은 여러 종류의 알고리즘을 순차적으로 실행할 수 있게 지원하는 고차원 API이며, 파이프 라인 API를 이용해 머신러닝을 위한 워크 플로우를 생성할 수 있습니다.

•파이프라인은 데이터 프레임을 사용합니다.

•Transformer – org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임을 변형해 새로운 데이터프레임을 생성하는 용도로 사용

•Estimator - org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머를 생성하는 역할을 합니다.

•Pipeline - org.apache.spark.ml 패키지에 선언된 클래스. 여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자. 하나의 파이프라인은 여러 개의 파이프라인 스테이지(PipelineStage)로 구성되며, 등록된 파이프라인 스테이지들은 우선순위에 따라 순차적으로 실행됩니다.

•ParamMap : 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스

```scala
scala> import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.feature.LabeledPoint

scala> import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.Vectors

scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]

scala> val v5 = LabeledPoint(1.0, v1)
v5: org.apache.spark.ml.feature.LabeledPoint = (1.0,[0.1,0.0,0.2,0.3])

scala> println(s"label:${v5.label}, features:${v5.features}")
label:1.0, features:[0.1,0.0,0.2,0.3]

scala> val training = spark.createDataFrame(Seq((161.0, 69.87, 29, 1.0),(176.78, 74.35, 34, 1.0),(159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender")
training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 2 more fields]

scala> training.cache()
res2: training.type = [height: double, weight: double ... 2 more fields]

scala> val test = spark.createDataFrame(Seq((169.4, 75.3, 42),(185.1, 85.0, 37),(161.6, 61.2, 28))).toDF("height", "weight", "age")
test: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 1 more field]

scala> training.show(false)
+------+------+---+------+
|height|weight|age|gender|
+------+------+---+------+
|161.0 |69.87 |29 |1.0   |
|176.78|74.35 |34 |1.0   |
|159.23|58.32 |29 |0.0   |
+------+------+---+------+

scala>  import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.feature.VectorAssembler

scala> val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")
assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7f26e6077190

scala> val assembled_training = assembler.transform(training)
assembled_training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 3 more fields]

scala> assemble
assembled_training   assembler

scala> assembled_training.show(false)
+------+------+---+------+-------------------+
|height|weight|age|gender|features           |
+------+------+---+------+-------------------+
|161.0 |69.87 |29 |1.0   |[161.0,69.87,29.0] |
|176.78|74.35 |34 |1.0   |[176.78,74.35,34.0]|
|159.23|58.32 |29 |0.0   |[159.23,58.32,29.0]|
+------+------+---+------+-------------------+


scala>  import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}

scala> val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01).setLabelCol("gender")
lr: org.apache.spark.ml.classification.LogisticRegression = logreg_db1a00187036

scala> val model = lr.fit(assembled_training)
19/09/05 18:40:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
19/09/05 18:40:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_db1a00187036, numClasses = 2, numFeatures = 3

scala> model.transform(assembled_training).show()    
+------+------+---+------+-------------------+--------------------+--------------------+----------+
|height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
+------+------+---+------+-------------------+--------------------+--------------------+----------+
| 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
|176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
|159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
+------+------+---+------+-------------------+--------------------+--------------------+----------+


scala>  import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.{Pipeline, PipelineModel}

scala> val pipeline = new Pipeline().setStages(Array(assembler, lr))    
pipeline: org.apache.spark.ml.Pipeline = pipeline_316c5e6fa0de

scala> val pipelineModel = pipeline.fit(training)    
pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_316c5e6fa0de

scala> pipelineModel.transform(training).show()    
+------+------+---+------+-------------------+--------------------+--------------------+----------+
|height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
+------+------+---+------+-------------------+--------------------+--------------------+----------+
| 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
|176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
|159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
+------+------+---+------+-------------------+--------------------+--------------------+----------+

scala> val path1 = "/output/sparkmllib/regression-model"    
path1: String = /output/sparkmllib/regression-model

scala> val path2 = "/output/sparkmllib/pipelinemodel"    
path2: String = /output/sparkmllib/pipelinemodel

scala> model.write.overwrite().save(path1)    
                                                                                
scala> pipelineModel.write.overwrite().save(path2)    

scala> val loadedModel = LogisticRegressionModel.load(path1)    
loadedModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_db1a00187036, numClasses = 2, numFeatures = 3

scala> val loadedPipelineModel = PipelineModel.load(path2)    
loadedPipelineModel: org.apache.spark.ml.PipelineModel = pipeline_316c5e6fa0de

scala> spark.stop

```

