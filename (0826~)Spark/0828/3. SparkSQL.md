## SparkSQL

- RDD는 데이터 값 자체는 표현이 가능하지만 데이터에 대한 메타데이터 스키마를 표현할 방법이 따로 없습니다.
- 스파크 SQL은 RDD의 메타 데이터 스키마를 표현할 수 있도록 데이터 모델과 API를 제공하는 스파크 모듈입니다.
- SQL을 작성할 때 ANSI-SQL과 Hive-QL 문법을 사용. 
- Spark 2.0부터 SQL:2003 표준 지원
- Spark 2.0부터 데이터셋이 스파크 SQL의 메인 API로 지정되면서 데이터 프레임은 사라지고 데이터셋으로 통합
- 데이터셋은 스키마를 기반으로 한 데이터 처리와 내부적인 성능 최적화를 함께 제공할 수 있다

 

### RDD와 DataFrame의 상호 변환

#### DataFrame 생성하기

- 케이스 클래스를 이용해 스키마 정보 부여해 DataFrame 작성

```scala
scala> case class Dessert(menuId :String, name: String, price:Int, kcal:Int)
defined class Dessert

scala> val dessertRDD =sc.textFile("/csv/dessert-menu.csv")
dessertRDD: org.apache.spark.rdd.RDD[String] = /csv/dessert-menu.csv MapPartitionsRDD[38] at textFile at <console>:30

scala> val dessertDF = dessertRDD.map { record =>
     | val splitRecord = record.split(",")
     | val menuId = splitRecord(0)
     | val name = splitRecord(1)
     | val price = splitRecord(2).toInt
     | val kcal = splitRecord(3).toInt
     | Dessert(menuId, name, price, kcal)
     | }.toDF
dessertDF: org.apache.spark.sql.DataFrame = [menuId: string, name: string ... 2 more fields]

scala> dessertDF.printSchema
root
 |-- menuId: string (nullable = true)
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)
 |-- kcal: integer (nullable = false)
```

#### DataFrame으로부터RDD 생성하기

`val rowRDD=dessertDF.rdd`

```scala
val nameAndPriceRDD = rowRDD.map { row =>
     | val name = row.getString(1)
     | val price = row.getInt(2)
     | (name,price)
     | }
nameAndPriceRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[44] at map at <console>:31

scala> nameAndPriceRDD.collect.foreach(println)
(초콜릿 파르페,4900)
(푸딩 파르페,5300)
(딸기 파르페,5200)
(판나코타,4200)
(치즈 무스,5800)
(아포가토,3000)
(티라미스,6000)
(녹차 파르페,4500)
(바닐라 젤라또,3600)
(카라멜 팬케익,3900)
(크림 안미츠,5000)
(고구마 파르페,6500)
(녹차 빙수,3800)
(초코 크레이프,3700)
(바나나 크레이프,3300)
(커스터드 푸딩,2000)
(초코 토르테,3300)
(치즈 수플레,2200)
(호박 타르트,3400)
(캬라멜 롤,3700)
(치즈 케익,4000)
(애플 파이,4400)
(몽블랑,4700)

```



#### DataFrame에 쿼리 발행하기

`dessertDF.registerTempTable("dessert_table")`

```scala
scala> val numOver300KcalDF =sqlContext.sql(
     | "select count(*) as num_of_over_300Kcal from dessert_table where kcal>=300")
numOver300KcalDF: org.apache.spark.sql.DataFrame = [num_of_over_300Kcal: bigint]
```

```scala
scala> numOver300KcalDF.show
+-------------------+                                                           
|num_of_over_300Kcal|
+-------------------+
|                  9|
+-------------------+
```

#### DataFrame API로 DataFrame 다루기

- 칼럼과 식 선택

  ```scala
  scala> val nameAndPriceDF =dessertDF.select(dessertDF("name"),dessertDF("price"))
  nameAndPriceDF: org.apache.spark.sql.DataFrame = [name: string, price: int]
  
  scala> nameAndPriceDF.printSchema
  root
   |-- name: string (nullable = true)
   |-- price: integer (nullable = false)
  ```

  

  ```
  scala> nameAndPriceDF.show
  +---------------+-----+
  |           name|price|
  +---------------+-----+
  |  초콜릿 파르페| 4900|
  |    푸딩 파르페| 5300|
  |    딸기 파르페| 5200|
  |       판나코타| 4200|
  |      치즈 무스| 5800|
  |       아포가토| 3000|
  |       티라미스| 6000|
  |    녹차 파르페| 4500|
  |  바닐라 젤라또| 3600|
  |  카라멜 팬케익| 3900|
  |    크림 안미츠| 5000|
  |  고구마 파르페| 6500|
  |      녹차 빙수| 3800|
  |  초코 크레이프| 3700|
  |바나나 크레이프| 3300|
  |  커스터드 푸딩| 2000|
  |    초코 토르테| 3300|
  |    치즈 수플레| 2200|
  |    호박 타르트| 3400|
  |      캬라멜 롤| 3700|
  +---------------+-----+
  only showing top 20 rows
  
  scala> selectAllDF.show(3)
  top3만 표시됨
  
  ```

- Dollar

  ```scala
  
  scala> val nameAndDollarDF =nameAndPriceDF.select($"name",$"price" /lit(1000.0))
  nameAndDollarDF: org.apache.spark.sql.DataFrame = [name: string, (price / 1000.0): double]
  
  scala> nameAndDollarDF.printSchema
  root
   |-- name: string (nullable = true)
   |-- (price / 1000.0): double (nullable = true)
  
  
  scala> val nameAndDollarDF = nameAndPriceDF.select(
       | $"name",($"price" /lit(1000.0)) as "dollar price")
  nameAndDollarDF: org.apache.spark.sql.DataFrame = [name: string, dollar price: double]
  ```

#### 필터링

- 5200원 이상 메뉴만 추출

  ```scala
  scala> val over5200WonDF = dessertDF.where($"price" >= 5200)
  over5200WonDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [menuId: string, name: string ... 2 more fields]
  
  scala> over5200WonDF.show
  +------+-------------+-----+----+
  |menuId|         name|price|kcal|
  +------+-------------+-----+----+
  |   D-1|  푸딩 파르페| 5300| 380|
  |   D-2|  딸기 파르페| 5200| 320|
  |   D-4|    치즈 무스| 5800| 288|
  |   D-6|     티라미스| 6000| 251|
  |  D-11|고구마 파르페| 6500| 650|
  +------+-------------+-----+----+
  ```

  

- 5200원 이상 메뉴의 name칼럼만 선택

  ```scala
  scala> val over5200WonDF = dessertDF.where($"price" >= 5200).select($"name")
  over5200WonDF: org.apache.spark.sql.DataFrame = [name: string]
  
  scala> over5200WonDF.show
  +-------------+
  |         name|
  +-------------+
  |  푸딩 파르페|
  |  딸기 파르페|
  |    치즈 무스|
  |     티라미스|
  |고구마 파르페|
  +-------------+
  ```

#### 정렬

- 정렬

  ```scala
  scala> val sortedDessertDF =dessertDF.orderBy($"price".asc, $"kcal".desc)
  sortedDessertDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [menuId: string, name: string ... 2 more fields]
  
  scala> sortedDessertDF.show
  +------+---------------+-----+----+
  |menuId|           name|price|kcal|
  +------+---------------+-----+----+
  |  D-15|  커스터드 푸딩| 2000| 120|
  |  D-17|    치즈 수플레| 2200| 160|
  |   D-5|       아포가토| 3000| 248|
  |  D-14|바나나 크레이프| 3300| 220|
  |  D-16|    초코 토르테| 3300| 220|
  |  D-18|    호박 타르트| 3400| 230|
  |   D-8|  바닐라 젤라또| 3600| 131|
  |  D-13|  초코 크레이프| 3700| 300|
  |  D-19|      캬라멜 롤| 3700| 230|
  |  D-12|      녹차 빙수| 3800| 320|
  |   D-9|  카라멜 팬케익| 3900| 440|
  |  D-20|      치즈 케익| 4000| 230|
  |   D-3|       판나코타| 4200| 283|
  |  D-21|      애플 파이| 4400| 350|
  |   D-7|    녹차 파르페| 4500| 380|
  |  D-22|         몽블랑| 4700| 290|
  |   D-0|  초콜릿 파르페| 4900| 420|
  |  D-10|    크림 안미츠| 5000| 250|
  |   D-2|    딸기 파르페| 5200| 320|
  |   D-1|    푸딩 파르페| 5300| 380|
  +------+---------------+-----+----+
  only showing top 20 rows
  ```

#### 집약처리

- 집야거

  ```scala
  scala> val avgKcalDF = dessertDF.agg(avg($"kcal") as "avg_of_kcal")
  avgKcalDF: org.apache.spark.sql.DataFrame = [avg_of_kcal: double]
  
  scala> avgKcalDF.show
  +-----------------+
  |      avg_of_kcal|
  +-----------------+
  |291.7826086956522|
  +-----------------+
  
  
  scala> import org.apache.spark.sql.types.DataTypes._
  import org.apache.spark.sql.types.DataTypes._
  
  scala> val numPerPriceRangeDF = dessertDF.groupBy (
       | (($"price"/1000) cast IntegerType ) * 1000
       | as "price_range").agg(count($"price")).orderBy($"price_range")
  numPerPriceRangeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [price_range: int, count(price): bigint]
  
  scala> num
  numOver300KcalDF   numPerPriceRangeDF
  
  scala> numPerPriceRangeDF.show
  +-----------+------------+                                                      
  |price_range|count(price)|
  +-----------+------------+
  |       2000|           2|
  |       3000|           9|
  |       4000|           6|
  |       5000|           4|
  |       6000|           2|
  +-----------+------------+
  ```

#### DataFrame끼리의 결합

- 조인

  ```scala
  scala> case class DessertOrder(sId:String, menuId:String,num:Int)
  defined class DessertOrder
  
  scala> val dessertOrderRDD = sc.textFile("/csv/dessert-order.csv")
  dessertOrderRDD: org.apache.spark.rdd.RDD[String] = /csv/dessert-order.csv MapPartitionsRDD[46] at textFile at <console>:34
  
  scala> val dessertOrderDF = dessertOrderRDD.map { record=>
       | val splitRecord = record.split(",")
       | val sId = splitRecord(0)
       | val menuId = splitRecord(1)
       | val num = splitRecord(2).toInt
       | DessertOrder(sId,menuId,num)
       | }.toDF
  dessertOrderDF: org.apache.spark.sql.DataFrame = [sId: string, menuId: string ... 1 more field]
  
  scala> val amntPerMenuPerSlipDF = 
       | dessertDF.join (
       | dessertOrderDF,
       | dessertDF("menuId") === dessertOrderDF("menuId"),
       | "inner"
       | ).select ($"sId",$"name", ($"num" * $"price")
       | as "amount_per_menu_per_slip")
  amntPerMenuPerSlipDF: org.apache.spark.sql.DataFrame = [sId: string, name: string ... 1 more field]
  
  scala> amntPerMenuPerSlipDF.show
  +-----+-------------+------------------------+                                  
  |  sId|         name|amount_per_menu_per_slip|
  +-----+-------------+------------------------+
  |SID-0|     판나코타|                    4200|
  |SID-2|     아포가토|                    3000|
  |SID-1|  크림 안미츠|                   20000|
  |SID-2|    치즈 케익|                    4000|
  |SID-2|바닐라 젤라또|                    3600|
  |SID-0|초콜릿 파르페|                    9800|
  +-----+-------------+------------------------+
  ```

- ㅇㅇ

  ```scala
  scala> val amntPerSlipDF = amntPerMenuPerSlipDF.groupBy($"sId").agg(
       | sum($"amount_per_menu_per_slip") as "amount_per_slip"
       | ).select($"sId",$"amount_per_slip")
  amntPerSlipDF: org.apache.spark.sql.DataFrame = [sId: string, amount_per_slip: bigint]
  
  scala> amntPer
  amntPerMenuPerSlipDF   amntPerSlipDF
  
  scala> amntPerPerSlipDF.show
  <console>:34: error: not found: value amntPerPerSlipDF
         amntPerPerSlipDF.show
         ^
  
  scala> amntPerSlipDF.show
  +-----+---------------+                                                         
  |  sId|amount_per_slip|
  +-----+---------------+
  |SID-0|          14000|
  |SID-1|          20000|
  |SID-2|          10600|
  +-----+---------------+
  ```

  

### UDF 이용하기

#### 스파크 SQL의 UDF 이용하기

- UDFRegistration - register()
  register( ___ , _____) 

  ```scala
  scala> val strlen =sqlContext.udf.register("strlen",(str:String)=>str.length)
  strlen: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))
  
  scala> sqlContext.sql("select strlen('Hello spark SQL') as result_of_strlen").show
  +----------------+
  |result_of_strlen|
  +----------------+
  |              15|
  +----------------+
  ```

  

### 구조화된 각종 데이터셋 다루기

#### 파일 형식의 구조화된 데이터셋 다루기

`scala> val dfWriter = dessertDF.write`

`scala> dfWriter.format("parquet").save("/csv/dessert_parquet")`



프로바이더가 존재하는 데이터셋으로부터 DataFrame 생성 -> <u>DataFrameReader</u>
	SQLContext.read로 얻을 수 있다.

`scala> val dfReader= sqlContext.read`



load메서드를 통해 파일에 대응하는 DataFrame을 얻음
	load(파일 시스템 경로 지정)
		이 시점에서 스키마 정보의 생성 관련 데이터는 읽지만, 데이터 전체는 읽지 않는다.

```scala
scala> val dessertDF2 = dfReader.format("parquet").load("/csv/dessert_parquet")
dessertDF2: org.apache.spark.sql.DataFrame = [menuId: string, name: string ... 2 more fields]

scala> dessertDF2.orderBy($"name").show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|  D-11|고구마 파르페| 6500| 650|
|  D-12|    녹차 빙수| 3800| 320|
|   D-7|  녹차 파르페| 4500| 380|
+------+-------------+-----+----+
only showing top 3 rows
```



#### 테이블 형식의 구조화된 데이터셋 다루기

- DataFrameWriter, DataFrameREader이용
  - DataFrame 내용을 파일에 출력할 때는 <u>save()</u>
    								  테이블에 출력할 때는 <u>saveAsTable("테이블 명 지정")</u>

`scala> dessertDF.write.format("parquet").saveAsTable("dessert_tbl_parquet")`



- 테이블로부터DataFrame 생성하려면 DataFrameReader 의 table메서드 이용

  ```scala
  scala> sqlContext.read.format("parquet").table("dessert_tbl_parquet").show(3)
  +------+-------------+-----+----+
  |menuId|         name|price|kcal|
  +------+-------------+-----+----+
  |   D-0|초콜릿 파르페| 4900| 420|
  |   D-1|  푸딩 파르페| 5300| 380|
  |   D-2|  딸기 파르페| 5200| 320|
  +------+-------------+-----+----+
  only showing top 3 rows
  ```

- 테이블 형식의 데이터셋을 읽어 들일 때는 DataFrameReader 대신 쿼리 발행

  ```scala
  scala> sqlContext.sql("select * from dessert_tbl_parquet limit 3").show
  +------+-------------+-----+----+
  |menuId|         name|price|kcal|
  +------+-------------+-----+----+
  |   D-0|초콜릿 파르페| 4900| 420|
  |   D-1|  푸딩 파르페| 5300| 380|
  |   D-2|  딸기 파르페| 5200| 320|
  +------+-------------+-----+----+
  ```

  