## Spark 설치

```
su - 
hadoop
cd /usr/local/
tar zxvf /home/hadoop/Downloads/spark-2.4.3-bin-hadoop2.7.tgz
//spark 홈페이지에서 받은 파일 해제
ls -l

ln -s  spark-2.4.3-bin-hadoop2.7  spark 
ls -l
chown -R hadoop:hadoop spark
ls -l

su hadoop
[hadoop@master ~]$ vi .bash_profile
#아래 내용 추가
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
source .bash_profile

spark-shell --master local verbose //spark shell 실행
```





//로컬 파일 시스템에서 파일을 읽어들여서 RDD로 생성
`val file=sc.textFile("file:///usr/local/spark/README.md")`

//RDD로 부터 한 행(라인)단위로 처리 -단어 분리 후 새로운 RDD 생성 저장
`val words = file.flatMap(_,split(" "))`

//같은 단어끼리 모아서 요약(개수) 계산 - 맵 형태로 단어와 출현횟수
`var result =words.countByValue`

`result.get("For").get`

```scala
scala> val file=sc.textFile("file:///usr/local/spark/README.md")
file: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at <console>:24

scala> val words = file.flatMap(_.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> var result =words.countByValue
result: scala.collection.Map[String,Long] = Map(site, -> 1, Please -> 4, Contributing -> 1, GraphX -> 1, project. -> 1, "" -> 72, for -> 12, find -> 1, Apache -> 1, package -> 1, Hadoop, -> 2, review -> 1, Once -> 1, For -> 3, name -> 1, this -> 1, protocols -> 1, Hive -> 2, in -> 6, "local[N]" -> 1, MASTER=spark://host:7077 -> 1, have -> 1, your -> 1, are -> 1, is -> 7, HDFS -> 1, Data. -> 1, built -> 1, thread, -> 1, examples -> 2, developing -> 1, using -> 5, system -> 1, than -> 1, Shell -> 2, mesos:// -> 1, 3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3). -> 1, easiest -> 1, This -> 2, -T -> 1, [Apache -> 1, N -> 1, integration -> 1, <class> -> 1, different -> 1, "local" -> 1, README -> 1, YARN"](http://spark.apache.org/docs/latest/building-spark.h...
scala> result.get("For").get
res0: Long = 3
```



### sbt 설치하기

압축해제

[hadoop@master ~]$ vi .bash_profile 	--bashprofile 내용추가
[hadoop@master ~]$ source .bash_profile
[hadoop@master ~]$ sbt about



### 스파크 어플리케이션 프로젝트 폴더 생성

[hadoop@master ~]$ mkdir spark-simple-app
[hadoop@master ~]$ cd spark-simple-app/

소스 코드 파일 저장 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala

sbt 설정파일 저장 디렉토리
[hadoop@master spark-simple-app]$ mkdir project

소스 코드  저장될 패키지 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example
[hadoop@master example]$ vi SundayCount.scala 

```scala
package c

import org.joda.time.{DateTime, DateTimeConstants}
import org.joda.time.format.DateTImeFormat
imprt org.apache.spark.{SparkConf,SparkContext}

object SundayCount{
def main (args:Array[String]) {
if (args.length<1){
throw new IllegalArgumentException (
"명령 인수에 날짜가 기록된 파일의 경로를 지정해 주세요.")
}
val filePath=args(0)
val conf= new SparkConf
val sc = new SparkContext(conf)

try{
//텍스트 파일을 로드한다
val textRDD=sc.textFile(fielePath)

val pattern= textRDD.map{dateStr=>
val pattern = DateTime,getDayOfWek == DateTimeConstants.SUNDAY
}

sundayRDD에 들어있는 일요일 개수를 센다
val numOfSunday = sundayRDD.count
println("주어진 데이터에는 일요일이 $(numOfSunday)개 들어 있습니다.")
}finally{
sc.stop()
}
}
}
```



