## Spark 설치

```
su - 
hadoop
cd /usr/local/
tar zxvf /home/hadoop/Downloads/spark-2.4.3-bin-hadoop2.7.tgz
//spark 홈페이지에서 받은 파일 해제
ls -l

ln -s  spark-2.4.3-bin-hadoop2.7  spark 
ls -l
chown -R hadoop:hadoop spark
ls -l

su hadoop
[hadoop@master ~]$ vi .bash_profile
#아래 내용 추가
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
source .bash_profile

spark-shell --master local verbose //spark shell 실행
```





//로컬 파일 시스템에서 파일을 읽어들여서 RDD로 생성
`val file=sc.textFile("file:///usr/local/spark/README.md")`

//RDD로 부터 한 행(라인)단위로 처리 -단어 분리 후 새로운 RDD 생성 저장
`val words = file.flatMap(_,split(" "))`

//같은 단어끼리 모아서 요약(개수) 계산 - 맵 형태로 단어와 출현횟수
`var result =words.countByValue`

`result.get("For").get`

```scala
scala> val file=sc.textFile("file:///usr/local/spark/README.md")
file: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at <console>:24

scala> val words = file.flatMap(_.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> var result =words.countByValue
result: scala.collection.Map[String,Long] = Map(site, -> 1, Please -> 4, Contributing -> 1, GraphX -> 1, project. -> 1, "" -> 72, for -> 12, find -> 1, Apache -> 1, package -> 1, Hadoop, -> 2, review -> 1, Once -> 1, For -> 3, name -> 1, this -> 1, protocols -> 1, Hive -> 2, in -> 6, "local[N]" -> 1, MASTER=spark://host:7077 -> 1, have -> 1, your -> 1, are -> 1, is -> 7, HDFS -> 1, Data. -> 1, built -> 1, thread, -> 1, examples -> 2, developing -> 1, using -> 5, system -> 1, than -> 1, Shell -> 2, mesos:// -> 1, 3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3). -> 1, easiest -> 1, This -> 2, -T -> 1, [Apache -> 1, N -> 1, integration -> 1, <class> -> 1, different -> 1, "local" -> 1, README -> 1, YARN"](http://spark.apache.org/docs/latest/building-spark.h...
scala> result.get("For").get
res0: Long = 3
```



### sbt 설치하기

압축해제

su -

`[hadoop@master ~]$ tar zxvf /home/hadoop/Downloads/sbt-1.2.7.tgz -C /opt/`

[hadoop@master ~]$ vi .bash_profile 	--bashprofile 내용추가 (export SBT_HOME=/opt/sbt)
[hadoop@master ~]$ source .bash_profile
[hadoop@master ~]$ sbt about



### 스파크 어플리케이션 프로젝트 폴더 생성

[hadoop@master ~]$ mkdir spark-simple-app
[hadoop@master ~]$ cd spark-simple-app/

소스 코드 파일 저장 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala

sbt 설정파일 저장 디렉토리
[hadoop@master spark-simple-app]$ mkdir project

소스 코드  저장될 패키지 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example
[hadoop@master example]$ vi SundayCount.scala 

```scala
package lab.spark.example 

import org.joda.time.{DateTime, DateTimeConstants}
import org.joda.time.format.DateTimeFormat
import org.apache.spark.{SparkConf,SparkContext}

object SundayCount{
def main (args:Array[String]) {
if (args.length<1){
throw new IllegalArgumentException (
"명령 인수에 날짜가 기록된 파일의 경로를 지정해 주세요.")
}
val filePath=args(0)
val conf= new SparkConf
val sc = new SparkContext(conf)

try{
//문자열로 표현된 날짜로부터 데이트타임형 인스턴스 생성
val textRDD=sc.textFile(filePath)
val dateTimeRDD =textRDD.map {dateStr =>
	val pattern =DateTimeFormat.forPattern("yyyyMMdd")
	DateTime.parse(dateStr,pattern)
}
//일요일만 추출한
val sundayRDD=dateTimeRDD.filter {dateTime =>
	dateTime.getDayOfWeek == DateTimeConstants.SUNDAY
}

//sundayRDD에 들어있는 일요일 개수를 센다
val numOfSunday = sundayRDD.count
	println(s"주어진 데이터에는 일요일이 ${numOfSunday}개 들어 있습니다.")
}finally{
sc.stop()
}
}
}
```



### build.sbt 작성

[hadoop@master example]$ cd ~/spark-simple-app
[hadoop@master spark-simple-app]$ vi build.sbt

```scala
name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided", "joda-time" % "joda-time" % "2.8.2")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)
```

### plugins.sbt 작성 & sbt-assembly 설정

```scala
[hadoop@master spark-simple-app]$ vi build.sbt
[hadoop@master spark-simple-app]$ cd project
[hadoop@master project]$ vi plugins.sbt
`addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")`
```

```scala
###어플리케이션 빌드

[hadoop@master project]$ cd ~/spark-simple-app
[hadoop@master spark-simple-app]$ sbt assembly
```

###어플리케이션 빌드 성공 결과
프로젝트 루트 디렉토리(~/spark-simple-app) 밑에 target 디렉토리 아래 scalar-2.11 디렉토리아래에 jar파일이 생성됨

`[hadoop@master ~]$ vi date.txt` ---날짜 데이터 입력

```scala
###하둡 파일 시스템에 date.txt파일 업로드

하둡을 켜고

[hadoop@master ~]$ hadoop fs -mkdir  /data/spark/
[hadoop@master ~]$ hadoop fs -put date.txt  /data/spark/
```

### spark-submit 

```scala
[hadoop@master ~]$ spark-submit --master local 
--class lab.spark.example.SundayCount 
--name SundayCount  
~/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  
/data/spark/date.txt
```

- spark-submit 
  --master <동작모드>
  --class <main 메서드가 구현된 애플리케이션의 클래스>
  --name <애플리케이션의 이름>
  <spark-submit 명령의 옵션>
  애플리케이션의 클래스가 포함된 JAR파일
  <애플리케이션에 넘기는 옵션>

---

## 스파크 RDD 기본액션

### collect

- RDD의 모든 원소를 모아서 배열로 돌려줌

- 파일이나 데이터베이스같은 외부 데이터를 이용하여 RDD생성

- RDD에 있는 모든 요소들이 collect연산을 호출한 서버의 메모리에 수집됨

- 전체 데이터를 모두 담을 수 있을 정도의 충분한 메모리 공간이 확보돼 있는 상태에서만 사용해야 한다.

  

- spark-shell --master local verbose 스파크에서 실행

  ```scala
  scala> val rdd=sc.parallelize(1 to 10)
  rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
  
  scala> val result = rdd.collect
  result: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
  
  scala> println(result.mkString(", "))
  1, 2, 3, 4, 5, 6, 7, 8, 9, 10
  
  scala> val result2 = rdd.count
  result2: Long = 10
  
  scala> println(result2)
  10
  ```

### Transformation

- Map 연산
  :	 요소 간의 사상을 정의한 함수 RDD에 속하는 모든 요소에 적용해 새로운 RDD 생상
- 그룹화연산
  : 특정 조건에 따라 요소를 그룹화하거나 특정함수를 적용
- 집합연산
  : RDD에 포함된 요소를 하나의 집합으로 간주할 때 서로 다른 RDD간에 합,교집합등을 계산
- 파티션연산
  : RDD파티션 갯수 조정
- 필터와 정렬 연산

### map

- 하나의 입력을 받아 하나의 값을 돌려주는 함수를 인자로 받음

- map 메서드 
  :`map[U](f:(T)=>U) : RDD [U]`

- RDD에 속하는 모든 요소에 적용한 뒤 그 결과로 구성된 새로운 RDD 반환

  ```scala
  scala> val rdd = sc.parallelize( 1 to 5)
  rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24
  
  scala> val result = rdd.map(_+1)
  result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:25
  
  scala> println (result.collect.mkString(", "))
  2, 3, 4, 5, 6
  ```

### flatmap

- TraversableOnce 는 스칼라에서 사용하는 이터레이터 타입 중 하나

- 함수 f는 반환 값으로 리스트나 시퀀스 같은 여러 개의 값을 담은 

  ```scala
  scala> val fruits = List("apple, orange", "grape, apple, mango","blueberry,tomato,orange")
  fruits: List[String] = List(apple, orange, grape, apple, mango, blueberry,tomato,orange)
  
  scala> val rdd1 = sc.parallelize (fruits)
  rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:26
  
  scala> val rdd2 = rdd1.flatMap(_.split(","))
  rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:25
  
  scala> print(rdd2)
  MapPartitionsRDD[4] at flatMap at <console>:25
  scala> print(rdd2.collect.mkString(", "))
  apple,  orange, grape,  apple,  mango, blueberry, tomato, orange
  
  ```

- some과 None은 값이 있거나 없을 수 있는 옵션 상황을 표시하는 스칼라 타입

- 값이 있다면 Some, 없으면 None

  ```scala
  scala> val rdd2 = rdd1.flatMap( log => {
       |   //apple이라는 단어가 포함된 경우만 처리 
       |   if(log.contains("apple")) {
       |      Some(log.indexOf("apple"))
       |   } else {
       |      None
       |   }
       |  })
  rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at flatMap at <console>:25
  ```

  ```scala
  scala> var rdd1=sc.parallelize( 1 to 10,3)
  rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:24
  
  scala> val rdd2=rdd1.mapPartitions
  mapPartitions   mapPartitionsWithIndex
  
  scala> val rdd2=rdd1.mapPartitions(numbers => {
       | print("DB Connected!!")
       | numbers.map{
       | number=> number+1
       | }
       | })
  rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at mapPartitions at <console>:25
  
  scala> println(rdd2.collect.mkString(", "))
  DB Connected!!DB Connected!!DB Connected!!2, 3, 4, 5, 6, 7, 8, 9, 10, 11
  
  ```

### mapPartitionWithIndex

- 인자로 전달받은 함수를 파티션 단위로 적용하고, 그 결과를 구성된 새로운 RDD

  ```scala
  scala> val rdd2= rdd1.mapPartitionsWithIndex((idx,numbers) => {
       | numbers.flatMap {
       | case number if idx ==1 =>Option(number +1)
       | case _ =>None
       | }
       | })
  rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[8] at mapPartitionsWithIndex at <console>:25
  
  scala> println(rdd2.collect.mkString(", "))
  5, 6, 7
  ```

### mapValue

- pairRDD에 속한느 데이터는 키를 기준으로 해서 작은 그룹들을 만들고 해당 그룹들에 속한 값을 대상으로 합계나 평균을 구하는 등의 연산 수행

- "key"에 해당하는 부분은 그대로 두고 "Value"에만 map() 연산 적용

- mapValue()

  ```scala
  scala> val rdd=sc.parallelize(List("a","b","c")).map((_,1))
  rdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:24
  
  scala> val result = rdd.mapValues(i => i+1)
  result: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[11] at mapValues at <console>:25
  
  scala> println(result.collect.mkString("\t"))
  (a,2)	(b,2)	(c,2)
  ```

### flatMapValues

- RDD의 모든 요소들이 키와 값의 쌍을 이루고 잇는 경우에만 사용 가능한 메서드

- "값"에 해당하는 요소에만 flatma() 연산을 적용하고 그 결과로 구성한 새로운 RDD 생성

  ```scala
  scala> val rdd = sc.parallelize(Seq((1, "a, b"), (2, "a, c"), (3, "d, e")))
  rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[12] at parallelize at <console>:24
  
  scala> val result = rdd.flatMapValues( _.split(","))
  result: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[13] at flatMapValues at <console>:25
  
  scala> println(result.collect.mkString("\t")) 
  (1,a)	(1, b)	(2,a)	(2, c)	(3,d)	(3, e)
  
  ```

### Zip

- 

  ```scala
  scala> val rdd1=sc.parallelize(List("a","b","c"))
  rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[16] at parallelize at <console>:24
  
  scala> val rdd2=sc.parallelize(List(1,2,3))
  rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:24
  
  scala> val result = rdd1.zip(rdd2)
  result: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[18] at zip at <console>:27
  
  scala> println(result.collect.mkString(", "))
  (a,1), (b,2), (c,3)
  
  ```

### groupBy

- RDD의 요소를 일정한 기준에 따라 여러 개의 그루븡로 나누고 이 그룹으로 구성된 새로운 RDD 생성

- 각 그룹은 키와 그 키에 속한 요소들의 시퀀스로 구성되며 , 메서드의 인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할을 담당

- 요소의 키를 생성하는 작업과 그룹으로 분류하는 작업을 동시에 수행

  ```scala
  scala> val rdd= sc.parallelize(1 to 10)
  rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at parallelize at <console>:24
  
  scala> val result = rdd.groupBy{
       | case i : Int if (i% 2 ==0) => "even"
       | case _ => 
       | "odd"
       | }
  result: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[21] at groupBy at <console>:25
  
  scala> result.collect.foreach{
       | v => println(s"${v._1}, [${v._2.mkString(", ")}]")
       | }
  even, [2, 4, 6, 8, 10]                                                          
  odd, [1, 3, 5, 7, 9]
  ```




### wordcount 

```
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-app

[hadoop@master ~]$ cd wordcount-app

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCount.scala


[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ sbt assembly

#데이터 소스 생성
[hadoop@master ~]$ vi simple-words.txt
cat
dog
.org
cat
rabbit
bear
cat
&&
tiger
dog
rabbit
100
bear
tiger
cat
rabbit
?bear

#하둡 파일 시스템에 simple-words.txt파일 업로드
[hadoop@master ~]$ hadoop fs -mkdir  /data/spark/
[hadoop@master ~]$ hadoop fs -put simple-words.txt  /data/spark/
 
[hadoop@master ~]$ spark-submit --master local 
--class lab.spark.example.WordCount 
--name WordCount  
~/wordcount-app/target/scala-2.11/wordcount-app-assembly-0.1.jar  
/data/spark/simple-words.txt

```



### groupByKey

### cogroup

- adf

  ```
  scala> val rdd = sc.parallelize( List(("k1", "v1"), ("k2", "v2"), ("k1", "v3") ))
  rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at <console>:24
  
  scala> val rdd2 = sc.parallelize(List(("k1", "v4")))
  rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[1] at parallelize at <console>:24
  
  scala> val result = rdd.cogroup(rdd2)
  result: org.apache.spark.rdd.RDD[(String, (Iterable[String], Iterable[String]))] = MapPartitionsRDD[3] at cogroup at <console>:27
  
  scala> result.collect.foreach {
       |    case (k, (v_1, v_2)) => {
       |         println(s"($k, [${v_1.mkString(",")}], [${v_2.mkString(", ")}])")
       |    }
       | }
  (k2, [v2], [])
  (k1, [v1,v3], [v4])
  
  ```

### distinct()

### Cartesian()

- 두 RDD 요소의 카테시안곱을 구하고 그 결과를 요소로 하는 새로운 RDD를 생성.

### substract()

- rdd1.substract(rdd2)는 rdd1에 속하고, rdd2에는 속하지 않는 요소로 구성된 새로운 RDD를 생성하는 메서드.

### intersection

### join

### left join, right join

- aasd

  ```scala
  scala> val rdd1 = sc.parallelize( List( "a", "a", "b", "c")).map((_, 1))
  rdd1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[2] at map at <console>:24
  
  scala> val rdd2 = sc.parallelize( List( "b", "c")).map((_, 1))
  rdd2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:24
  
  scala> val result1 = rdd1.leftOuterJoin(rdd2)
  result1: org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[7] at leftOuterJoin at <console>:27
  
  scala> val result2 = rdd1.rightOuterJoin(rdd2)
  result2: org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[10] at rightOuterJoin at <console>:27
  
  scala> println("Left:" + result1.collect.mkString("\t"))
  Left:(a,(1,None))       (a,(1,None))    (b,(1,Some(1))) (c,(1,Some(1)))         
  
  scala> println("Right:" + result2.collect.mkString("\t"))
  Right:(b,(Some(1),1))	(c,(Some(1),1))
  
  ```

  

### reduceByKey

- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드

- 같은 키를 가진 값들을 하나로 병합해 키-값 쌍으로 구성된 새로운 RDD를 생성합니다.

- 병합을 수행하기 위해 두 개의 값을 하나로 합치는 함수를 인자로 전달받는데 , 함수가 수행하는 연산은 데이터가 여러 파티션에 분산돼 있어야 항상 같은 순서로 연산을 보장할 수 있도록 결합법칙과 교환법칙이 성립됨을 보장해야 합니다.

  ```scala
  val rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
  val result = rdd.reduceByKey(rdd)
  println(result.collect.mkString(","))
  ```

  

### foldByKey

- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드

- 같은 키를 가진 값을 하나로 병합해 키-값 쌍으로 구성된 새로운 RDD를 반환, 병합 연산의 초기 값을 메서드의 인자로 전달해서 병합 시 사용할 수 있다

- 초기값은 여러 번 반복 사용해도 연산 결과에는 영향을 주지 않는 값이어야 합니다.

  ```scala
  val rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
  val result = rdd.foldByKey(_+_)
  println(result.collect.mkString(","))
  ```

### combineByKey

- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드
- 같은 키를 가진 값들을 하나로 병합하는 기능을 수행하는 과정에서 값의 타입이 바뀔 수 있다
  - createCombiner() 
    : 값을 병합하기 위한 콤바이너를 생성, 두 개의 값을 하나로 병합하는 객체
  - mergeValue() 
    : 키에 대한 콤바이너가 이미 존재한다면 새로운 콤바이너를 만들지 않고 이 함수를 이용해 값을 기존 콤바이너에 병합시킵니다.
  - mergeCombiner()
    : createCombiner() 와 mergeValue() 는 파티션 단위로 수행됩니다. 
      mergeCombiner()는 병합이 끝난 콤바이너들끼리 다시 병합을 수행해 최종 콤바이너를 생성합니다.

- 에제 - 평균 구하기

  ```scala
  //콤바이너 역할을 할 Record 클래스 정의
  scala> case class Record(var amount: Long, var number: Long=1){
       |    def map(v: Long) = Record(v)
       |    def add(amount: Long): Record = {
       |       add(map(amount))
       |     }
       |    def add(other: Record) : Record = {
       |       this.number +=other.number 
       |     this.amount += other.amount
       |     this
       |     }
       |    override def toString: String = s"avg:${amount / number}"
       |    }
  defined class Record
  
  
  scala>  val data = Seq(("Math", 100L), ("Eng", 80L), ("Math", 50L), ("Eng", 60L), ("Eng", 90L))
  data: Seq[(String, Long)] = List((Math,100), (Eng,80), (Math,50), (Eng,60), (Eng,90))
  
  scala>  val rdd = sc.parallelize(data)
  rdd: org.apache.spark.rdd.RDD[(String, Long)] = ParallelCollectionRDD[11] at parallelize at <console>:26
  
  scala>  val createCombiner = (v:Long) => Record(v)
  createCombiner: Long => Record = <function1>
  
  scala> val mergeValue = (c:Record, v:Long) => c.add(v)
  mergeValue: (Record, Long) => Record = <function2>
  
  scala> val mergeCombiners = (c1:Record, c2:Record) => c1.add(c2)
  mergeCombiners: (Record, Record) => Record = <function2>
  
  scala>  val result = rdd.combineByKey(createCombiner, mergeValue, mergeCombiners)
  result: org.apache.spark.rdd.RDD[(String, Record)] = ShuffledRDD[12] at combineByKey at <console>:31
  
  scala>  println(result.collect.mkString("\n"))
  (Math,avg:75)
  (Eng,avg:76)
  
  ```

  

